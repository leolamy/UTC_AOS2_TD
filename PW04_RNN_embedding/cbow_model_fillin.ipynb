{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW model trained on 20000 lieues sous les mers\n",
    "\n",
    "## Needed libraries\n",
    "\n",
    "You will need the following new libraries:\n",
    "\n",
    "-   `spacy` for tokenizing\n",
    "-   `gensim` for cosine similarities (use `gensim>=4.0.0`)\n",
    "\n",
    "You will also need to download rules for tokenizing a french text.\n",
    "\n",
    "``` bash\n",
    "python -m spacy download fr_core_news_sm\n",
    "```"
   ],
   "id": "a8c36c4a-f827-4d80-815a-1a63a52d533f"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:32:04.605263Z",
     "start_time": "2025-12-03T10:32:04.548407Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ],
   "id": "16caf452"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the corpus"
   ],
   "id": "2d4d23d4-fb42-4ae2-a463-d26baae65dd6"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.7.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.3/16.3 MB\u001B[0m \u001B[31m39.9 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from fr-core-news-sm==3.7.0) (3.7.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.4)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.15)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.11)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.2)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.67.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.5)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.12.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.6)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (80.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (25.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.5.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.41.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.15.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2025.11.12)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.3.1)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (5.2.1)\r\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "# Installe directement le modèle français depuis le dépôt officiel\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-03T10:32:06.489989Z",
     "start_time": "2025-12-03T10:32:04.551465Z"
    }
   },
   "id": "5b8e54d0592e20aa"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:32:08.695361Z",
     "start_time": "2025-12-03T10:32:06.492655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use a french tokenizer to create a tokenizer for the french language\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "with open(\"data/20_000_lieues_sous_les_mers.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = spacy_fr.tokenizer(f.read())\n",
    "\n",
    "# Define a filtered set of tokens by iterating on `document`. Define a\n",
    "# subset of tokens that are\n",
    "#\n",
    "# - alphanumeric\n",
    "# - in lower case\n",
    "tokens = [\n",
    "    token.text.lower()\n",
    "    for token in document if token.is_alpha or token.is_digit and not token.is_space and not len(token.text) > 2 and not token.is_stop\n",
    "]\n",
    "\n",
    "# Make a list of unique tokens and dictionary that maps tokens to\n",
    "# their index in that list.\n",
    "idx2tok = list(set(tokens))\n",
    "tok2idx = {token: idx for idx, token in enumerate(idx2tok)}"
   ],
   "id": "cb7760ad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The continuous bag of words model"
   ],
   "id": "51ea7838-5112-4156-b3b9-fda846cf3905"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:32:08.736331Z",
     "start_time": "2025-12-03T10:32:08.698605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "CBOW(\n  (embeddings): Embedding(14558, 128)\n  (U_transpose): Linear(in_features=128, out_features=14558, bias=False)\n)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Define an Embedding module (`nn.Embedding`) and a linear\n",
    "        # transform (`nn.Linear`) without bias.\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.U_transpose = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Implements the forward pass of the CBOW model\n",
    "        # `context` is of size `batch_size` * NGRAMS\n",
    "\n",
    "        # `e_i` is of size `batch_size` * NGRAMS * `embedding_size`\n",
    "        e_i = self.embeddings(context)\n",
    "\n",
    "        # `e_bar` is of size `batch_size` * `embedding_size`\n",
    "        e_bar = torch.mean(e_i, dim=1)\n",
    "\n",
    "        # `UT_e_bar` is of size `batch_size` * `vocab_size`\n",
    "        UT_e_bar = self.U_transpose(e_bar)\n",
    "\n",
    "        return UT_e_bar\n",
    "\n",
    "\n",
    "# Set the size of vocabulary and size of embedding\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "# Create a Continuous bag of words model\n",
    "cbow = CBOW(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "\n",
    "# Send to GPU if any\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "cbow.to(device)"
   ],
   "id": "5629a0c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ],
   "id": "9b713dc9-ec5d-498f-bbc8-46cf87ae17c5"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:32:09.117166Z",
     "start_time": "2025-12-03T10:32:09.007529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate n-grams for a given list of tokens, use yield, use window length of n-grams\n",
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Generates successive N-grams from a list of tokens.\"\"\"\n",
    "\n",
    "    for i in range(len(token_list) - ngrams + 1):\n",
    "        idxs = [tok2idx[tok] for tok in token_list[i:i+ngrams]]\n",
    "\n",
    "        # Get center element in `idxs`\n",
    "        center = idxs.pop(ngrams // 2)\n",
    "\n",
    "        # Yield the index of center word and indexes of context words\n",
    "        # as a Numpy array (for Pytorch to automatically convert it to\n",
    "        # a Tensor).\n",
    "        yield center, np.array(idxs)\n",
    "\n",
    "\n",
    "# Create center, context data\n",
    "NGRAMS = 9\n",
    "ngrams = list(ngrams_iterator(tokens, NGRAMS))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "data = torch.utils.data.DataLoader(ngrams, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "id": "0ad3f5cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn CBOW model"
   ],
   "id": "5485e26c-e1c3-488f-b829-85b5f75da5b8"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:32:09.124382Z",
     "start_time": "2025-12-03T10:32:09.117858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient descent algorithm to use\n",
    "optimizer = optim.Adam(cbow.parameters(), lr=0.01)\n",
    "\n",
    "# Use a cross-entropy loss from the `nn` submodule\n",
    "ce_loss = nn.CrossEntropyLoss()"
   ],
   "id": "87c0389a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:39:02.126038Z",
     "start_time": "2025-12-03T10:32:09.120891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/50), batch: (0/271), loss: 9.589542388916016\n",
      "Epoch (1/50), batch: (20/271), loss: 8.639681816101074\n",
      "Epoch (1/50), batch: (40/271), loss: 8.157389640808105\n",
      "Epoch (1/50), batch: (60/271), loss: 7.898660659790039\n",
      "Epoch (1/50), batch: (80/271), loss: 7.733100891113281\n",
      "Epoch (1/50), batch: (100/271), loss: 7.601471900939941\n",
      "Epoch (1/50), batch: (120/271), loss: 7.50297212600708\n",
      "Epoch (1/50), batch: (140/271), loss: 7.430117130279541\n",
      "Epoch (1/50), batch: (160/271), loss: 7.369608402252197\n",
      "Epoch (1/50), batch: (180/271), loss: 7.31596040725708\n",
      "Epoch (1/50), batch: (200/271), loss: 7.260052680969238\n",
      "Epoch (1/50), batch: (220/271), loss: 7.209349632263184\n",
      "Epoch (1/50), batch: (240/271), loss: 7.1690568923950195\n",
      "Epoch (1/50), batch: (260/271), loss: 7.133075714111328\n",
      "1/50 loss 7.12\n",
      "Epoch (2/50), batch: (0/271), loss: 5.37155818939209\n",
      "Epoch (2/50), batch: (20/271), loss: 5.359081268310547\n",
      "Epoch (2/50), batch: (40/271), loss: 5.3762946128845215\n",
      "Epoch (2/50), batch: (60/271), loss: 5.385535717010498\n",
      "Epoch (2/50), batch: (80/271), loss: 5.399145603179932\n",
      "Epoch (2/50), batch: (100/271), loss: 5.401846408843994\n",
      "Epoch (2/50), batch: (120/271), loss: 5.402938365936279\n",
      "Epoch (2/50), batch: (140/271), loss: 5.403980731964111\n",
      "Epoch (2/50), batch: (160/271), loss: 5.403226852416992\n",
      "Epoch (2/50), batch: (180/271), loss: 5.40792179107666\n",
      "Epoch (2/50), batch: (200/271), loss: 5.412901401519775\n",
      "Epoch (2/50), batch: (220/271), loss: 5.417398929595947\n",
      "Epoch (2/50), batch: (240/271), loss: 5.41648530960083\n",
      "Epoch (2/50), batch: (260/271), loss: 5.415988922119141\n",
      "2/50 loss 5.42\n",
      "Epoch (3/50), batch: (0/271), loss: 4.136436462402344\n",
      "Epoch (3/50), batch: (20/271), loss: 4.059926509857178\n",
      "Epoch (3/50), batch: (40/271), loss: 4.0919694900512695\n",
      "Epoch (3/50), batch: (60/271), loss: 4.121356010437012\n",
      "Epoch (3/50), batch: (80/271), loss: 4.143135070800781\n",
      "Epoch (3/50), batch: (100/271), loss: 4.1630706787109375\n",
      "Epoch (3/50), batch: (120/271), loss: 4.181803226470947\n",
      "Epoch (3/50), batch: (140/271), loss: 4.204661846160889\n",
      "Epoch (3/50), batch: (160/271), loss: 4.2223076820373535\n",
      "Epoch (3/50), batch: (180/271), loss: 4.238529205322266\n",
      "Epoch (3/50), batch: (200/271), loss: 4.256169319152832\n",
      "Epoch (3/50), batch: (220/271), loss: 4.269191265106201\n",
      "Epoch (3/50), batch: (240/271), loss: 4.279537200927734\n",
      "Epoch (3/50), batch: (260/271), loss: 4.292176723480225\n",
      "3/50 loss 4.30\n",
      "Epoch (4/50), batch: (0/271), loss: 3.067110300064087\n",
      "Epoch (4/50), batch: (20/271), loss: 3.1278457641601562\n",
      "Epoch (4/50), batch: (40/271), loss: 3.139145612716675\n",
      "Epoch (4/50), batch: (60/271), loss: 3.1646931171417236\n",
      "Epoch (4/50), batch: (80/271), loss: 3.182486057281494\n",
      "Epoch (4/50), batch: (100/271), loss: 3.2116024494171143\n",
      "Epoch (4/50), batch: (120/271), loss: 3.236732244491577\n",
      "Epoch (4/50), batch: (140/271), loss: 3.2592086791992188\n",
      "Epoch (4/50), batch: (160/271), loss: 3.280864715576172\n",
      "Epoch (4/50), batch: (180/271), loss: 3.2957403659820557\n",
      "Epoch (4/50), batch: (200/271), loss: 3.3136470317840576\n",
      "Epoch (4/50), batch: (220/271), loss: 3.331204891204834\n",
      "Epoch (4/50), batch: (240/271), loss: 3.346390962600708\n",
      "Epoch (4/50), batch: (260/271), loss: 3.3638734817504883\n",
      "4/50 loss 3.37\n",
      "Epoch (5/50), batch: (0/271), loss: 2.517997980117798\n",
      "Epoch (5/50), batch: (20/271), loss: 2.4382646083831787\n",
      "Epoch (5/50), batch: (40/271), loss: 2.4615983963012695\n",
      "Epoch (5/50), batch: (60/271), loss: 2.475740432739258\n",
      "Epoch (5/50), batch: (80/271), loss: 2.4953582286834717\n",
      "Epoch (5/50), batch: (100/271), loss: 2.514322519302368\n",
      "Epoch (5/50), batch: (120/271), loss: 2.5404491424560547\n",
      "Epoch (5/50), batch: (140/271), loss: 2.5587942600250244\n",
      "Epoch (5/50), batch: (160/271), loss: 2.581118583679199\n",
      "Epoch (5/50), batch: (180/271), loss: 2.6007461547851562\n",
      "Epoch (5/50), batch: (200/271), loss: 2.6187984943389893\n",
      "Epoch (5/50), batch: (220/271), loss: 2.6338460445404053\n",
      "Epoch (5/50), batch: (240/271), loss: 2.651554822921753\n",
      "Epoch (5/50), batch: (260/271), loss: 2.664138078689575\n",
      "5/50 loss 2.67\n",
      "Epoch (6/50), batch: (0/271), loss: 1.964665412902832\n",
      "Epoch (6/50), batch: (20/271), loss: 1.9290060997009277\n",
      "Epoch (6/50), batch: (40/271), loss: 1.9353797435760498\n",
      "Epoch (6/50), batch: (60/271), loss: 1.9509505033493042\n",
      "Epoch (6/50), batch: (80/271), loss: 1.9692727327346802\n",
      "Epoch (6/50), batch: (100/271), loss: 1.9822421073913574\n",
      "Epoch (6/50), batch: (120/271), loss: 2.000537872314453\n",
      "Epoch (6/50), batch: (140/271), loss: 2.0129294395446777\n",
      "Epoch (6/50), batch: (160/271), loss: 2.0302340984344482\n",
      "Epoch (6/50), batch: (180/271), loss: 2.045924663543701\n",
      "Epoch (6/50), batch: (200/271), loss: 2.063730001449585\n",
      "Epoch (6/50), batch: (220/271), loss: 2.0795187950134277\n",
      "Epoch (6/50), batch: (240/271), loss: 2.0962109565734863\n",
      "Epoch (6/50), batch: (260/271), loss: 2.110968589782715\n",
      "6/50 loss 2.12\n",
      "Epoch (7/50), batch: (0/271), loss: 1.4967187643051147\n",
      "Epoch (7/50), batch: (20/271), loss: 1.498915195465088\n",
      "Epoch (7/50), batch: (40/271), loss: 1.5204397439956665\n",
      "Epoch (7/50), batch: (60/271), loss: 1.528005838394165\n",
      "Epoch (7/50), batch: (80/271), loss: 1.5448448657989502\n",
      "Epoch (7/50), batch: (100/271), loss: 1.5599150657653809\n",
      "Epoch (7/50), batch: (120/271), loss: 1.5837472677230835\n",
      "Epoch (7/50), batch: (140/271), loss: 1.6015855073928833\n",
      "Epoch (7/50), batch: (160/271), loss: 1.6161212921142578\n",
      "Epoch (7/50), batch: (180/271), loss: 1.6302356719970703\n",
      "Epoch (7/50), batch: (200/271), loss: 1.6458922624588013\n",
      "Epoch (7/50), batch: (220/271), loss: 1.659567952156067\n",
      "Epoch (7/50), batch: (240/271), loss: 1.6726514101028442\n",
      "Epoch (7/50), batch: (260/271), loss: 1.6841092109680176\n",
      "7/50 loss 1.69\n",
      "Epoch (8/50), batch: (0/271), loss: 1.225512146949768\n",
      "Epoch (8/50), batch: (20/271), loss: 1.2182190418243408\n",
      "Epoch (8/50), batch: (40/271), loss: 1.2116104364395142\n",
      "Epoch (8/50), batch: (60/271), loss: 1.2272530794143677\n",
      "Epoch (8/50), batch: (80/271), loss: 1.2435139417648315\n",
      "Epoch (8/50), batch: (100/271), loss: 1.2663689851760864\n",
      "Epoch (8/50), batch: (120/271), loss: 1.2804721593856812\n",
      "Epoch (8/50), batch: (140/271), loss: 1.2884756326675415\n",
      "Epoch (8/50), batch: (160/271), loss: 1.3000037670135498\n",
      "Epoch (8/50), batch: (180/271), loss: 1.3135216236114502\n",
      "Epoch (8/50), batch: (200/271), loss: 1.326440453529358\n",
      "Epoch (8/50), batch: (220/271), loss: 1.3382835388183594\n",
      "Epoch (8/50), batch: (240/271), loss: 1.3481922149658203\n",
      "Epoch (8/50), batch: (260/271), loss: 1.3612329959869385\n",
      "8/50 loss 1.37\n",
      "Epoch (9/50), batch: (0/271), loss: 0.8565496802330017\n",
      "Epoch (9/50), batch: (20/271), loss: 0.9660526514053345\n",
      "Epoch (9/50), batch: (40/271), loss: 0.9919428825378418\n",
      "Epoch (9/50), batch: (60/271), loss: 1.0042670965194702\n",
      "Epoch (9/50), batch: (80/271), loss: 1.013883113861084\n",
      "Epoch (9/50), batch: (100/271), loss: 1.0257693529129028\n",
      "Epoch (9/50), batch: (120/271), loss: 1.037998080253601\n",
      "Epoch (9/50), batch: (140/271), loss: 1.0502475500106812\n",
      "Epoch (9/50), batch: (160/271), loss: 1.0609841346740723\n",
      "Epoch (9/50), batch: (180/271), loss: 1.0740803480148315\n",
      "Epoch (9/50), batch: (200/271), loss: 1.0837050676345825\n",
      "Epoch (9/50), batch: (220/271), loss: 1.0944010019302368\n",
      "Epoch (9/50), batch: (240/271), loss: 1.105391025543213\n",
      "Epoch (9/50), batch: (260/271), loss: 1.1152985095977783\n",
      "9/50 loss 1.12\n",
      "Epoch (10/50), batch: (0/271), loss: 0.7286571860313416\n",
      "Epoch (10/50), batch: (20/271), loss: 0.7695492506027222\n",
      "Epoch (10/50), batch: (40/271), loss: 0.7945345640182495\n",
      "Epoch (10/50), batch: (60/271), loss: 0.8033493161201477\n",
      "Epoch (10/50), batch: (80/271), loss: 0.8163809776306152\n",
      "Epoch (10/50), batch: (100/271), loss: 0.8319147825241089\n",
      "Epoch (10/50), batch: (120/271), loss: 0.8423694968223572\n",
      "Epoch (10/50), batch: (140/271), loss: 0.8552196025848389\n",
      "Epoch (10/50), batch: (160/271), loss: 0.8671600818634033\n",
      "Epoch (10/50), batch: (180/271), loss: 0.878663957118988\n",
      "Epoch (10/50), batch: (200/271), loss: 0.8920035362243652\n",
      "Epoch (10/50), batch: (220/271), loss: 0.9044620394706726\n",
      "Epoch (10/50), batch: (240/271), loss: 0.9165129661560059\n",
      "Epoch (10/50), batch: (260/271), loss: 0.9266366958618164\n",
      "10/50 loss 0.93\n",
      "Epoch (11/50), batch: (0/271), loss: 0.6213817596435547\n",
      "Epoch (11/50), batch: (20/271), loss: 0.6635939478874207\n",
      "Epoch (11/50), batch: (40/271), loss: 0.6722551584243774\n",
      "Epoch (11/50), batch: (60/271), loss: 0.6821149587631226\n",
      "Epoch (11/50), batch: (80/271), loss: 0.689736008644104\n",
      "Epoch (11/50), batch: (100/271), loss: 0.7015968561172485\n",
      "Epoch (11/50), batch: (120/271), loss: 0.7089132070541382\n",
      "Epoch (11/50), batch: (140/271), loss: 0.7208415269851685\n",
      "Epoch (11/50), batch: (160/271), loss: 0.7353665232658386\n",
      "Epoch (11/50), batch: (180/271), loss: 0.7453923225402832\n",
      "Epoch (11/50), batch: (200/271), loss: 0.7539508938789368\n",
      "Epoch (11/50), batch: (220/271), loss: 0.763786792755127\n",
      "Epoch (11/50), batch: (240/271), loss: 0.7733029127120972\n",
      "Epoch (11/50), batch: (260/271), loss: 0.7825610637664795\n",
      "11/50 loss 0.79\n",
      "Epoch (12/50), batch: (0/271), loss: 0.5940146446228027\n",
      "Epoch (12/50), batch: (20/271), loss: 0.5801834464073181\n",
      "Epoch (12/50), batch: (40/271), loss: 0.5837774276733398\n",
      "Epoch (12/50), batch: (60/271), loss: 0.5903962850570679\n",
      "Epoch (12/50), batch: (80/271), loss: 0.5992013812065125\n",
      "Epoch (12/50), batch: (100/271), loss: 0.6072261333465576\n",
      "Epoch (12/50), batch: (120/271), loss: 0.6107757687568665\n",
      "Epoch (12/50), batch: (140/271), loss: 0.6181772351264954\n",
      "Epoch (12/50), batch: (160/271), loss: 0.6280415654182434\n",
      "Epoch (12/50), batch: (180/271), loss: 0.6369898319244385\n",
      "Epoch (12/50), batch: (200/271), loss: 0.644543468952179\n",
      "Epoch (12/50), batch: (220/271), loss: 0.652226984500885\n",
      "Epoch (12/50), batch: (240/271), loss: 0.6606160402297974\n",
      "Epoch (12/50), batch: (260/271), loss: 0.6673155426979065\n",
      "12/50 loss 0.67\n",
      "Epoch (13/50), batch: (0/271), loss: 0.46056485176086426\n",
      "Epoch (13/50), batch: (20/271), loss: 0.4847162663936615\n",
      "Epoch (13/50), batch: (40/271), loss: 0.49137306213378906\n",
      "Epoch (13/50), batch: (60/271), loss: 0.5036134123802185\n",
      "Epoch (13/50), batch: (80/271), loss: 0.5117745399475098\n",
      "Epoch (13/50), batch: (100/271), loss: 0.5174000263214111\n",
      "Epoch (13/50), batch: (120/271), loss: 0.5230496525764465\n",
      "Epoch (13/50), batch: (140/271), loss: 0.5299997925758362\n",
      "Epoch (13/50), batch: (160/271), loss: 0.5370036959648132\n",
      "Epoch (13/50), batch: (180/271), loss: 0.545100748538971\n",
      "Epoch (13/50), batch: (200/271), loss: 0.5541198253631592\n",
      "Epoch (13/50), batch: (220/271), loss: 0.5630201697349548\n",
      "Epoch (13/50), batch: (240/271), loss: 0.5713351964950562\n",
      "Epoch (13/50), batch: (260/271), loss: 0.5778277516365051\n",
      "13/50 loss 0.58\n",
      "Epoch (14/50), batch: (0/271), loss: 0.4708729386329651\n",
      "Epoch (14/50), batch: (20/271), loss: 0.43579351902008057\n",
      "Epoch (14/50), batch: (40/271), loss: 0.42559391260147095\n",
      "Epoch (14/50), batch: (60/271), loss: 0.4317918121814728\n",
      "Epoch (14/50), batch: (80/271), loss: 0.438419908285141\n",
      "Epoch (14/50), batch: (100/271), loss: 0.4461883306503296\n",
      "Epoch (14/50), batch: (120/271), loss: 0.4543524384498596\n",
      "Epoch (14/50), batch: (140/271), loss: 0.4614797830581665\n",
      "Epoch (14/50), batch: (160/271), loss: 0.4695279002189636\n",
      "Epoch (14/50), batch: (180/271), loss: 0.47775667905807495\n",
      "Epoch (14/50), batch: (200/271), loss: 0.4848305284976959\n",
      "Epoch (14/50), batch: (220/271), loss: 0.492016464471817\n",
      "Epoch (14/50), batch: (240/271), loss: 0.4981606900691986\n",
      "Epoch (14/50), batch: (260/271), loss: 0.5046186447143555\n",
      "14/50 loss 0.51\n",
      "Epoch (15/50), batch: (0/271), loss: 0.3526684045791626\n",
      "Epoch (15/50), batch: (20/271), loss: 0.3732852339744568\n",
      "Epoch (15/50), batch: (40/271), loss: 0.37093815207481384\n",
      "Epoch (15/50), batch: (60/271), loss: 0.37449535727500916\n",
      "Epoch (15/50), batch: (80/271), loss: 0.38597404956817627\n",
      "Epoch (15/50), batch: (100/271), loss: 0.3936062455177307\n",
      "Epoch (15/50), batch: (120/271), loss: 0.4019699990749359\n",
      "Epoch (15/50), batch: (140/271), loss: 0.4073801636695862\n",
      "Epoch (15/50), batch: (160/271), loss: 0.4113526940345764\n",
      "Epoch (15/50), batch: (180/271), loss: 0.417384535074234\n",
      "Epoch (15/50), batch: (200/271), loss: 0.42305395007133484\n",
      "Epoch (15/50), batch: (220/271), loss: 0.4312811493873596\n",
      "Epoch (15/50), batch: (240/271), loss: 0.4389551877975464\n",
      "Epoch (15/50), batch: (260/271), loss: 0.4441666901111603\n",
      "15/50 loss 0.45\n",
      "Epoch (16/50), batch: (0/271), loss: 0.26742780208587646\n",
      "Epoch (16/50), batch: (20/271), loss: 0.32679229974746704\n",
      "Epoch (16/50), batch: (40/271), loss: 0.32826122641563416\n",
      "Epoch (16/50), batch: (60/271), loss: 0.3303714394569397\n",
      "Epoch (16/50), batch: (80/271), loss: 0.3373371660709381\n",
      "Epoch (16/50), batch: (100/271), loss: 0.34376612305641174\n",
      "Epoch (16/50), batch: (120/271), loss: 0.3504151701927185\n",
      "Epoch (16/50), batch: (140/271), loss: 0.35628542304039\n",
      "Epoch (16/50), batch: (160/271), loss: 0.36378213763237\n",
      "Epoch (16/50), batch: (180/271), loss: 0.3697269856929779\n",
      "Epoch (16/50), batch: (200/271), loss: 0.37503936886787415\n",
      "Epoch (16/50), batch: (220/271), loss: 0.3808327913284302\n",
      "Epoch (16/50), batch: (240/271), loss: 0.3872646689414978\n",
      "Epoch (16/50), batch: (260/271), loss: 0.392809122800827\n",
      "16/50 loss 0.40\n",
      "Epoch (17/50), batch: (0/271), loss: 0.2482050508260727\n",
      "Epoch (17/50), batch: (20/271), loss: 0.28758421540260315\n",
      "Epoch (17/50), batch: (40/271), loss: 0.29162856936454773\n",
      "Epoch (17/50), batch: (60/271), loss: 0.29433563351631165\n",
      "Epoch (17/50), batch: (80/271), loss: 0.3017929792404175\n",
      "Epoch (17/50), batch: (100/271), loss: 0.30686256289482117\n",
      "Epoch (17/50), batch: (120/271), loss: 0.3117623031139374\n",
      "Epoch (17/50), batch: (140/271), loss: 0.3169114291667938\n",
      "Epoch (17/50), batch: (160/271), loss: 0.3211975693702698\n",
      "Epoch (17/50), batch: (180/271), loss: 0.32760903239250183\n",
      "Epoch (17/50), batch: (200/271), loss: 0.33288517594337463\n",
      "Epoch (17/50), batch: (220/271), loss: 0.33997854590415955\n",
      "Epoch (17/50), batch: (240/271), loss: 0.34570521116256714\n",
      "Epoch (17/50), batch: (260/271), loss: 0.35176751017570496\n",
      "17/50 loss 0.35\n",
      "Epoch (18/50), batch: (0/271), loss: 0.2273475080728531\n",
      "Epoch (18/50), batch: (20/271), loss: 0.2416384518146515\n",
      "Epoch (18/50), batch: (40/271), loss: 0.24911586940288544\n",
      "Epoch (18/50), batch: (60/271), loss: 0.2587701082229614\n",
      "Epoch (18/50), batch: (80/271), loss: 0.26738840341567993\n",
      "Epoch (18/50), batch: (100/271), loss: 0.27164965867996216\n",
      "Epoch (18/50), batch: (120/271), loss: 0.2756882309913635\n",
      "Epoch (18/50), batch: (140/271), loss: 0.28127461671829224\n",
      "Epoch (18/50), batch: (160/271), loss: 0.2867838740348816\n",
      "Epoch (18/50), batch: (180/271), loss: 0.2919899821281433\n",
      "Epoch (18/50), batch: (200/271), loss: 0.29734402894973755\n",
      "Epoch (18/50), batch: (220/271), loss: 0.30268293619155884\n",
      "Epoch (18/50), batch: (240/271), loss: 0.3089573085308075\n",
      "Epoch (18/50), batch: (260/271), loss: 0.3157818615436554\n",
      "18/50 loss 0.32\n",
      "Epoch (19/50), batch: (0/271), loss: 0.1918833702802658\n",
      "Epoch (19/50), batch: (20/271), loss: 0.23143945634365082\n",
      "Epoch (19/50), batch: (40/271), loss: 0.23714642226696014\n",
      "Epoch (19/50), batch: (60/271), loss: 0.24062024056911469\n",
      "Epoch (19/50), batch: (80/271), loss: 0.24545374512672424\n",
      "Epoch (19/50), batch: (100/271), loss: 0.2492683082818985\n",
      "Epoch (19/50), batch: (120/271), loss: 0.252928763628006\n",
      "Epoch (19/50), batch: (140/271), loss: 0.25770848989486694\n",
      "Epoch (19/50), batch: (160/271), loss: 0.2625226676464081\n",
      "Epoch (19/50), batch: (180/271), loss: 0.2663361430168152\n",
      "Epoch (19/50), batch: (200/271), loss: 0.27113839983940125\n",
      "Epoch (19/50), batch: (220/271), loss: 0.2767990529537201\n",
      "Epoch (19/50), batch: (240/271), loss: 0.2813745141029358\n",
      "Epoch (19/50), batch: (260/271), loss: 0.2859337627887726\n",
      "19/50 loss 0.29\n",
      "Epoch (20/50), batch: (0/271), loss: 0.23111781477928162\n",
      "Epoch (20/50), batch: (20/271), loss: 0.2013135701417923\n",
      "Epoch (20/50), batch: (40/271), loss: 0.21141080558300018\n",
      "Epoch (20/50), batch: (60/271), loss: 0.2113608419895172\n",
      "Epoch (20/50), batch: (80/271), loss: 0.21608807146549225\n",
      "Epoch (20/50), batch: (100/271), loss: 0.22149375081062317\n",
      "Epoch (20/50), batch: (120/271), loss: 0.22408588230609894\n",
      "Epoch (20/50), batch: (140/271), loss: 0.22947636246681213\n",
      "Epoch (20/50), batch: (160/271), loss: 0.23360005021095276\n",
      "Epoch (20/50), batch: (180/271), loss: 0.24084162712097168\n",
      "Epoch (20/50), batch: (200/271), loss: 0.24589262902736664\n",
      "Epoch (20/50), batch: (220/271), loss: 0.2489021271467209\n",
      "Epoch (20/50), batch: (240/271), loss: 0.2552163004875183\n",
      "Epoch (20/50), batch: (260/271), loss: 0.2599641680717468\n",
      "20/50 loss 0.26\n",
      "Epoch (21/50), batch: (0/271), loss: 0.15800751745700836\n",
      "Epoch (21/50), batch: (20/271), loss: 0.1905343383550644\n",
      "Epoch (21/50), batch: (40/271), loss: 0.19524134695529938\n",
      "Epoch (21/50), batch: (60/271), loss: 0.19814398884773254\n",
      "Epoch (21/50), batch: (80/271), loss: 0.19835670292377472\n",
      "Epoch (21/50), batch: (100/271), loss: 0.20105788111686707\n",
      "Epoch (21/50), batch: (120/271), loss: 0.2073911428451538\n",
      "Epoch (21/50), batch: (140/271), loss: 0.21148116886615753\n",
      "Epoch (21/50), batch: (160/271), loss: 0.21618680655956268\n",
      "Epoch (21/50), batch: (180/271), loss: 0.21967503428459167\n",
      "Epoch (21/50), batch: (200/271), loss: 0.22542010247707367\n",
      "Epoch (21/50), batch: (220/271), loss: 0.22985394299030304\n",
      "Epoch (21/50), batch: (240/271), loss: 0.23386193811893463\n",
      "Epoch (21/50), batch: (260/271), loss: 0.23786689341068268\n",
      "21/50 loss 0.24\n",
      "Epoch (22/50), batch: (0/271), loss: 0.20125989615917206\n",
      "Epoch (22/50), batch: (20/271), loss: 0.16880162060260773\n",
      "Epoch (22/50), batch: (40/271), loss: 0.17496342957019806\n",
      "Epoch (22/50), batch: (60/271), loss: 0.1791101098060608\n",
      "Epoch (22/50), batch: (80/271), loss: 0.18346664309501648\n",
      "Epoch (22/50), batch: (100/271), loss: 0.1856195479631424\n",
      "Epoch (22/50), batch: (120/271), loss: 0.19025501608848572\n",
      "Epoch (22/50), batch: (140/271), loss: 0.19299528002738953\n",
      "Epoch (22/50), batch: (160/271), loss: 0.19720149040222168\n",
      "Epoch (22/50), batch: (180/271), loss: 0.20141549408435822\n",
      "Epoch (22/50), batch: (200/271), loss: 0.2064158171415329\n",
      "Epoch (22/50), batch: (220/271), loss: 0.21069961786270142\n",
      "Epoch (22/50), batch: (240/271), loss: 0.21612779796123505\n",
      "Epoch (22/50), batch: (260/271), loss: 0.21909013390541077\n",
      "22/50 loss 0.22\n",
      "Epoch (23/50), batch: (0/271), loss: 0.1650935709476471\n",
      "Epoch (23/50), batch: (20/271), loss: 0.1612185686826706\n",
      "Epoch (23/50), batch: (40/271), loss: 0.16320262849330902\n",
      "Epoch (23/50), batch: (60/271), loss: 0.1666516661643982\n",
      "Epoch (23/50), batch: (80/271), loss: 0.17039963603019714\n",
      "Epoch (23/50), batch: (100/271), loss: 0.17214930057525635\n",
      "Epoch (23/50), batch: (120/271), loss: 0.17429690062999725\n",
      "Epoch (23/50), batch: (140/271), loss: 0.1789218634366989\n",
      "Epoch (23/50), batch: (160/271), loss: 0.18319852650165558\n",
      "Epoch (23/50), batch: (180/271), loss: 0.18724536895751953\n",
      "Epoch (23/50), batch: (200/271), loss: 0.19032502174377441\n",
      "Epoch (23/50), batch: (220/271), loss: 0.19374650716781616\n",
      "Epoch (23/50), batch: (240/271), loss: 0.19877085089683533\n",
      "Epoch (23/50), batch: (260/271), loss: 0.20311042666435242\n",
      "23/50 loss 0.20\n",
      "Epoch (24/50), batch: (0/271), loss: 0.14202497899532318\n",
      "Epoch (24/50), batch: (20/271), loss: 0.15224364399909973\n",
      "Epoch (24/50), batch: (40/271), loss: 0.15705662965774536\n",
      "Epoch (24/50), batch: (60/271), loss: 0.1548829972743988\n",
      "Epoch (24/50), batch: (80/271), loss: 0.1558724045753479\n",
      "Epoch (24/50), batch: (100/271), loss: 0.16018716990947723\n",
      "Epoch (24/50), batch: (120/271), loss: 0.16196198761463165\n",
      "Epoch (24/50), batch: (140/271), loss: 0.1657782644033432\n",
      "Epoch (24/50), batch: (160/271), loss: 0.16929227113723755\n",
      "Epoch (24/50), batch: (180/271), loss: 0.17196504771709442\n",
      "Epoch (24/50), batch: (200/271), loss: 0.17633669078350067\n",
      "Epoch (24/50), batch: (220/271), loss: 0.18047067523002625\n",
      "Epoch (24/50), batch: (240/271), loss: 0.1841951310634613\n",
      "Epoch (24/50), batch: (260/271), loss: 0.18707449734210968\n",
      "24/50 loss 0.19\n",
      "Epoch (25/50), batch: (0/271), loss: 0.09595955908298492\n",
      "Epoch (25/50), batch: (20/271), loss: 0.12530046701431274\n",
      "Epoch (25/50), batch: (40/271), loss: 0.13155417144298553\n",
      "Epoch (25/50), batch: (60/271), loss: 0.1422846019268036\n",
      "Epoch (25/50), batch: (80/271), loss: 0.14372211694717407\n",
      "Epoch (25/50), batch: (100/271), loss: 0.14509090781211853\n",
      "Epoch (25/50), batch: (120/271), loss: 0.14591126143932343\n",
      "Epoch (25/50), batch: (140/271), loss: 0.150628924369812\n",
      "Epoch (25/50), batch: (160/271), loss: 0.1549961417913437\n",
      "Epoch (25/50), batch: (180/271), loss: 0.15785382688045502\n",
      "Epoch (25/50), batch: (200/271), loss: 0.16180716454982758\n",
      "Epoch (25/50), batch: (220/271), loss: 0.16507282853126526\n",
      "Epoch (25/50), batch: (240/271), loss: 0.16868841648101807\n",
      "Epoch (25/50), batch: (260/271), loss: 0.17365403473377228\n",
      "25/50 loss 0.18\n",
      "Epoch (26/50), batch: (0/271), loss: 0.09064693003892899\n",
      "Epoch (26/50), batch: (20/271), loss: 0.12797243893146515\n",
      "Epoch (26/50), batch: (40/271), loss: 0.12973491847515106\n",
      "Epoch (26/50), batch: (60/271), loss: 0.13017357885837555\n",
      "Epoch (26/50), batch: (80/271), loss: 0.13434553146362305\n",
      "Epoch (26/50), batch: (100/271), loss: 0.13746383786201477\n",
      "Epoch (26/50), batch: (120/271), loss: 0.14017260074615479\n",
      "Epoch (26/50), batch: (140/271), loss: 0.1435282826423645\n",
      "Epoch (26/50), batch: (160/271), loss: 0.14663243293762207\n",
      "Epoch (26/50), batch: (180/271), loss: 0.15015944838523865\n",
      "Epoch (26/50), batch: (200/271), loss: 0.1532999724149704\n",
      "Epoch (26/50), batch: (220/271), loss: 0.15637442469596863\n",
      "Epoch (26/50), batch: (240/271), loss: 0.15962575376033783\n",
      "Epoch (26/50), batch: (260/271), loss: 0.16232438385486603\n",
      "26/50 loss 0.16\n",
      "Epoch (27/50), batch: (0/271), loss: 0.0908646360039711\n",
      "Epoch (27/50), batch: (20/271), loss: 0.110938660800457\n",
      "Epoch (27/50), batch: (40/271), loss: 0.11037348210811615\n",
      "Epoch (27/50), batch: (60/271), loss: 0.1148160770535469\n",
      "Epoch (27/50), batch: (80/271), loss: 0.12007609754800797\n",
      "Epoch (27/50), batch: (100/271), loss: 0.1259961873292923\n",
      "Epoch (27/50), batch: (120/271), loss: 0.13068877160549164\n",
      "Epoch (27/50), batch: (140/271), loss: 0.13331317901611328\n",
      "Epoch (27/50), batch: (160/271), loss: 0.13555830717086792\n",
      "Epoch (27/50), batch: (180/271), loss: 0.139318585395813\n",
      "Epoch (27/50), batch: (200/271), loss: 0.14143143594264984\n",
      "Epoch (27/50), batch: (220/271), loss: 0.144379660487175\n",
      "Epoch (27/50), batch: (240/271), loss: 0.147918701171875\n",
      "Epoch (27/50), batch: (260/271), loss: 0.15148364007472992\n",
      "27/50 loss 0.15\n",
      "Epoch (28/50), batch: (0/271), loss: 0.10602030903100967\n",
      "Epoch (28/50), batch: (20/271), loss: 0.10017776489257812\n",
      "Epoch (28/50), batch: (40/271), loss: 0.10667243599891663\n",
      "Epoch (28/50), batch: (60/271), loss: 0.10815972834825516\n",
      "Epoch (28/50), batch: (80/271), loss: 0.11414272338151932\n",
      "Epoch (28/50), batch: (100/271), loss: 0.1184580996632576\n",
      "Epoch (28/50), batch: (120/271), loss: 0.12105006724596024\n",
      "Epoch (28/50), batch: (140/271), loss: 0.12363264709711075\n",
      "Epoch (28/50), batch: (160/271), loss: 0.1279202401638031\n",
      "Epoch (28/50), batch: (180/271), loss: 0.129886656999588\n",
      "Epoch (28/50), batch: (200/271), loss: 0.13301163911819458\n",
      "Epoch (28/50), batch: (220/271), loss: 0.1357322782278061\n",
      "Epoch (28/50), batch: (240/271), loss: 0.13906383514404297\n",
      "Epoch (28/50), batch: (260/271), loss: 0.14178459346294403\n",
      "28/50 loss 0.14\n",
      "Epoch (29/50), batch: (0/271), loss: 0.12143819779157639\n",
      "Epoch (29/50), batch: (20/271), loss: 0.0954366996884346\n",
      "Epoch (29/50), batch: (40/271), loss: 0.10101531445980072\n",
      "Epoch (29/50), batch: (60/271), loss: 0.10337282717227936\n",
      "Epoch (29/50), batch: (80/271), loss: 0.10375795513391495\n",
      "Epoch (29/50), batch: (100/271), loss: 0.10908901691436768\n",
      "Epoch (29/50), batch: (120/271), loss: 0.11114417016506195\n",
      "Epoch (29/50), batch: (140/271), loss: 0.11422836035490036\n",
      "Epoch (29/50), batch: (160/271), loss: 0.11641538143157959\n",
      "Epoch (29/50), batch: (180/271), loss: 0.11931838095188141\n",
      "Epoch (29/50), batch: (200/271), loss: 0.1229129433631897\n",
      "Epoch (29/50), batch: (220/271), loss: 0.12616786360740662\n",
      "Epoch (29/50), batch: (240/271), loss: 0.12954391539096832\n",
      "Epoch (29/50), batch: (260/271), loss: 0.13262243568897247\n",
      "29/50 loss 0.13\n",
      "Epoch (30/50), batch: (0/271), loss: 0.06578942388296127\n",
      "Epoch (30/50), batch: (20/271), loss: 0.09499812871217728\n",
      "Epoch (30/50), batch: (40/271), loss: 0.09692990779876709\n",
      "Epoch (30/50), batch: (60/271), loss: 0.09877486526966095\n",
      "Epoch (30/50), batch: (80/271), loss: 0.1007407158613205\n",
      "Epoch (30/50), batch: (100/271), loss: 0.10328934341669083\n",
      "Epoch (30/50), batch: (120/271), loss: 0.10521499067544937\n",
      "Epoch (30/50), batch: (140/271), loss: 0.10840030759572983\n",
      "Epoch (30/50), batch: (160/271), loss: 0.11076204478740692\n",
      "Epoch (30/50), batch: (180/271), loss: 0.11267676949501038\n",
      "Epoch (30/50), batch: (200/271), loss: 0.1161578819155693\n",
      "Epoch (30/50), batch: (220/271), loss: 0.1195584237575531\n",
      "Epoch (30/50), batch: (240/271), loss: 0.1222233772277832\n",
      "Epoch (30/50), batch: (260/271), loss: 0.12590555846691132\n",
      "30/50 loss 0.13\n",
      "Epoch (31/50), batch: (0/271), loss: 0.07829246670007706\n",
      "Epoch (31/50), batch: (20/271), loss: 0.09089776128530502\n",
      "Epoch (31/50), batch: (40/271), loss: 0.08923324942588806\n",
      "Epoch (31/50), batch: (60/271), loss: 0.09093119204044342\n",
      "Epoch (31/50), batch: (80/271), loss: 0.09463804960250854\n",
      "Epoch (31/50), batch: (100/271), loss: 0.0984264686703682\n",
      "Epoch (31/50), batch: (120/271), loss: 0.10160545259714127\n",
      "Epoch (31/50), batch: (140/271), loss: 0.10397405177354813\n",
      "Epoch (31/50), batch: (160/271), loss: 0.10588638484477997\n",
      "Epoch (31/50), batch: (180/271), loss: 0.10851811617612839\n",
      "Epoch (31/50), batch: (200/271), loss: 0.11192821711301804\n",
      "Epoch (31/50), batch: (220/271), loss: 0.1150486096739769\n",
      "Epoch (31/50), batch: (240/271), loss: 0.11825168132781982\n",
      "Epoch (31/50), batch: (260/271), loss: 0.1205156147480011\n",
      "31/50 loss 0.12\n",
      "Epoch (32/50), batch: (0/271), loss: 0.087928906083107\n",
      "Epoch (32/50), batch: (20/271), loss: 0.08258518576622009\n",
      "Epoch (32/50), batch: (40/271), loss: 0.08830620348453522\n",
      "Epoch (32/50), batch: (60/271), loss: 0.09035487473011017\n",
      "Epoch (32/50), batch: (80/271), loss: 0.09199296683073044\n",
      "Epoch (32/50), batch: (100/271), loss: 0.09406276047229767\n",
      "Epoch (32/50), batch: (120/271), loss: 0.09622127562761307\n",
      "Epoch (32/50), batch: (140/271), loss: 0.09789330512285233\n",
      "Epoch (32/50), batch: (160/271), loss: 0.0998438224196434\n",
      "Epoch (32/50), batch: (180/271), loss: 0.10208386182785034\n",
      "Epoch (32/50), batch: (200/271), loss: 0.1044151708483696\n",
      "Epoch (32/50), batch: (220/271), loss: 0.10736552625894547\n",
      "Epoch (32/50), batch: (240/271), loss: 0.1108371764421463\n",
      "Epoch (32/50), batch: (260/271), loss: 0.11341582983732224\n",
      "32/50 loss 0.12\n",
      "Epoch (33/50), batch: (0/271), loss: 0.08661863207817078\n",
      "Epoch (33/50), batch: (20/271), loss: 0.07944405823945999\n",
      "Epoch (33/50), batch: (40/271), loss: 0.08183830976486206\n",
      "Epoch (33/50), batch: (60/271), loss: 0.08517809957265854\n",
      "Epoch (33/50), batch: (80/271), loss: 0.0868307426571846\n",
      "Epoch (33/50), batch: (100/271), loss: 0.08937130123376846\n",
      "Epoch (33/50), batch: (120/271), loss: 0.09229324012994766\n",
      "Epoch (33/50), batch: (140/271), loss: 0.09455812722444534\n",
      "Epoch (33/50), batch: (160/271), loss: 0.09594021737575531\n",
      "Epoch (33/50), batch: (180/271), loss: 0.09819377213716507\n",
      "Epoch (33/50), batch: (200/271), loss: 0.10077491402626038\n",
      "Epoch (33/50), batch: (220/271), loss: 0.10396155714988708\n",
      "Epoch (33/50), batch: (240/271), loss: 0.1067582294344902\n",
      "Epoch (33/50), batch: (260/271), loss: 0.10938841849565506\n",
      "33/50 loss 0.11\n",
      "Epoch (34/50), batch: (0/271), loss: 0.06993860751390457\n",
      "Epoch (34/50), batch: (20/271), loss: 0.07995877414941788\n",
      "Epoch (34/50), batch: (40/271), loss: 0.08066967874765396\n",
      "Epoch (34/50), batch: (60/271), loss: 0.08247599750757217\n",
      "Epoch (34/50), batch: (80/271), loss: 0.0808355063199997\n",
      "Epoch (34/50), batch: (100/271), loss: 0.08291099965572357\n",
      "Epoch (34/50), batch: (120/271), loss: 0.08575104922056198\n",
      "Epoch (34/50), batch: (140/271), loss: 0.08794821053743362\n",
      "Epoch (34/50), batch: (160/271), loss: 0.09028930217027664\n",
      "Epoch (34/50), batch: (180/271), loss: 0.09346582740545273\n",
      "Epoch (34/50), batch: (200/271), loss: 0.09596675634384155\n",
      "Epoch (34/50), batch: (220/271), loss: 0.09870809316635132\n",
      "Epoch (34/50), batch: (240/271), loss: 0.10166456550359726\n",
      "Epoch (34/50), batch: (260/271), loss: 0.1043107882142067\n",
      "34/50 loss 0.11\n",
      "Epoch (35/50), batch: (0/271), loss: 0.07747256010770798\n",
      "Epoch (35/50), batch: (20/271), loss: 0.07651189714670181\n",
      "Epoch (35/50), batch: (40/271), loss: 0.0766916424036026\n",
      "Epoch (35/50), batch: (60/271), loss: 0.08024606853723526\n",
      "Epoch (35/50), batch: (80/271), loss: 0.08218694478273392\n",
      "Epoch (35/50), batch: (100/271), loss: 0.08506108075380325\n",
      "Epoch (35/50), batch: (120/271), loss: 0.08714068681001663\n",
      "Epoch (35/50), batch: (140/271), loss: 0.08830279111862183\n",
      "Epoch (35/50), batch: (160/271), loss: 0.08992838114500046\n",
      "Epoch (35/50), batch: (180/271), loss: 0.09091885387897491\n",
      "Epoch (35/50), batch: (200/271), loss: 0.0930975005030632\n",
      "Epoch (35/50), batch: (220/271), loss: 0.09638293832540512\n",
      "Epoch (35/50), batch: (240/271), loss: 0.09850703924894333\n",
      "Epoch (35/50), batch: (260/271), loss: 0.10114611685276031\n",
      "35/50 loss 0.10\n",
      "Epoch (36/50), batch: (0/271), loss: 0.06781582534313202\n",
      "Epoch (36/50), batch: (20/271), loss: 0.07082106173038483\n",
      "Epoch (36/50), batch: (40/271), loss: 0.07674729824066162\n",
      "Epoch (36/50), batch: (60/271), loss: 0.07894022762775421\n",
      "Epoch (36/50), batch: (80/271), loss: 0.07941500097513199\n",
      "Epoch (36/50), batch: (100/271), loss: 0.08154882490634918\n",
      "Epoch (36/50), batch: (120/271), loss: 0.0829293355345726\n",
      "Epoch (36/50), batch: (140/271), loss: 0.08481810241937637\n",
      "Epoch (36/50), batch: (160/271), loss: 0.08590003848075867\n",
      "Epoch (36/50), batch: (180/271), loss: 0.08766646683216095\n",
      "Epoch (36/50), batch: (200/271), loss: 0.09037692844867706\n",
      "Epoch (36/50), batch: (220/271), loss: 0.09215429425239563\n",
      "Epoch (36/50), batch: (240/271), loss: 0.09461435675621033\n",
      "Epoch (36/50), batch: (260/271), loss: 0.09711664915084839\n",
      "36/50 loss 0.10\n",
      "Epoch (37/50), batch: (0/271), loss: 0.056977059692144394\n",
      "Epoch (37/50), batch: (20/271), loss: 0.06629929691553116\n",
      "Epoch (37/50), batch: (40/271), loss: 0.067221999168396\n",
      "Epoch (37/50), batch: (60/271), loss: 0.06814225763082504\n",
      "Epoch (37/50), batch: (80/271), loss: 0.07071225345134735\n",
      "Epoch (37/50), batch: (100/271), loss: 0.07354029268026352\n",
      "Epoch (37/50), batch: (120/271), loss: 0.07554826885461807\n",
      "Epoch (37/50), batch: (140/271), loss: 0.07844433188438416\n",
      "Epoch (37/50), batch: (160/271), loss: 0.08002161234617233\n",
      "Epoch (37/50), batch: (180/271), loss: 0.08177036792039871\n",
      "Epoch (37/50), batch: (200/271), loss: 0.08379612863063812\n",
      "Epoch (37/50), batch: (220/271), loss: 0.08594164997339249\n",
      "Epoch (37/50), batch: (240/271), loss: 0.08806271851062775\n",
      "Epoch (37/50), batch: (260/271), loss: 0.09042657911777496\n",
      "37/50 loss 0.09\n",
      "Epoch (38/50), batch: (0/271), loss: 0.07700072228908539\n",
      "Epoch (38/50), batch: (20/271), loss: 0.07280489057302475\n",
      "Epoch (38/50), batch: (40/271), loss: 0.07205984741449356\n",
      "Epoch (38/50), batch: (60/271), loss: 0.07029078900814056\n",
      "Epoch (38/50), batch: (80/271), loss: 0.07100550830364227\n",
      "Epoch (38/50), batch: (100/271), loss: 0.07287148386240005\n",
      "Epoch (38/50), batch: (120/271), loss: 0.0739462599158287\n",
      "Epoch (38/50), batch: (140/271), loss: 0.07551519572734833\n",
      "Epoch (38/50), batch: (160/271), loss: 0.07739263772964478\n",
      "Epoch (38/50), batch: (180/271), loss: 0.0785856544971466\n",
      "Epoch (38/50), batch: (200/271), loss: 0.08076421916484833\n",
      "Epoch (38/50), batch: (220/271), loss: 0.08281020820140839\n",
      "Epoch (38/50), batch: (240/271), loss: 0.08451571315526962\n",
      "Epoch (38/50), batch: (260/271), loss: 0.08628662675619125\n",
      "38/50 loss 0.09\n",
      "Epoch (39/50), batch: (0/271), loss: 0.0368066281080246\n",
      "Epoch (39/50), batch: (20/271), loss: 0.06033739447593689\n",
      "Epoch (39/50), batch: (40/271), loss: 0.06471174210309982\n",
      "Epoch (39/50), batch: (60/271), loss: 0.0632217675447464\n",
      "Epoch (39/50), batch: (80/271), loss: 0.06632688641548157\n",
      "Epoch (39/50), batch: (100/271), loss: 0.0680486410856247\n",
      "Epoch (39/50), batch: (120/271), loss: 0.06955192983150482\n",
      "Epoch (39/50), batch: (140/271), loss: 0.07118155807256699\n",
      "Epoch (39/50), batch: (160/271), loss: 0.07307413220405579\n",
      "Epoch (39/50), batch: (180/271), loss: 0.075197733938694\n",
      "Epoch (39/50), batch: (200/271), loss: 0.07669065147638321\n",
      "Epoch (39/50), batch: (220/271), loss: 0.07810624688863754\n",
      "Epoch (39/50), batch: (240/271), loss: 0.08059488236904144\n",
      "Epoch (39/50), batch: (260/271), loss: 0.08264937996864319\n",
      "39/50 loss 0.08\n",
      "Epoch (40/50), batch: (0/271), loss: 0.06255771219730377\n",
      "Epoch (40/50), batch: (20/271), loss: 0.05847647786140442\n",
      "Epoch (40/50), batch: (40/271), loss: 0.06048949807882309\n",
      "Epoch (40/50), batch: (60/271), loss: 0.06044119969010353\n",
      "Epoch (40/50), batch: (80/271), loss: 0.062332481145858765\n",
      "Epoch (40/50), batch: (100/271), loss: 0.06452210992574692\n",
      "Epoch (40/50), batch: (120/271), loss: 0.06641504168510437\n",
      "Epoch (40/50), batch: (140/271), loss: 0.06731526553630829\n",
      "Epoch (40/50), batch: (160/271), loss: 0.06951134651899338\n",
      "Epoch (40/50), batch: (180/271), loss: 0.0716763362288475\n",
      "Epoch (40/50), batch: (200/271), loss: 0.07354021072387695\n",
      "Epoch (40/50), batch: (220/271), loss: 0.0756155252456665\n",
      "Epoch (40/50), batch: (240/271), loss: 0.07814668864011765\n",
      "Epoch (40/50), batch: (260/271), loss: 0.08048724383115768\n",
      "40/50 loss 0.08\n",
      "Epoch (41/50), batch: (0/271), loss: 0.038615986704826355\n",
      "Epoch (41/50), batch: (20/271), loss: 0.055932749062776566\n",
      "Epoch (41/50), batch: (40/271), loss: 0.061616450548172\n",
      "Epoch (41/50), batch: (60/271), loss: 0.06183963268995285\n",
      "Epoch (41/50), batch: (80/271), loss: 0.062460757791996\n",
      "Epoch (41/50), batch: (100/271), loss: 0.06294280290603638\n",
      "Epoch (41/50), batch: (120/271), loss: 0.06594310700893402\n",
      "Epoch (41/50), batch: (140/271), loss: 0.06699646264314651\n",
      "Epoch (41/50), batch: (160/271), loss: 0.06743285059928894\n",
      "Epoch (41/50), batch: (180/271), loss: 0.068976029753685\n",
      "Epoch (41/50), batch: (200/271), loss: 0.07064811885356903\n",
      "Epoch (41/50), batch: (220/271), loss: 0.0726841613650322\n",
      "Epoch (41/50), batch: (240/271), loss: 0.07515271753072739\n",
      "Epoch (41/50), batch: (260/271), loss: 0.07765475660562515\n",
      "41/50 loss 0.08\n",
      "Epoch (42/50), batch: (0/271), loss: 0.05067009478807449\n",
      "Epoch (42/50), batch: (20/271), loss: 0.05707967281341553\n",
      "Epoch (42/50), batch: (40/271), loss: 0.05900955945253372\n",
      "Epoch (42/50), batch: (60/271), loss: 0.05913843214511871\n",
      "Epoch (42/50), batch: (80/271), loss: 0.06037801131606102\n",
      "Epoch (42/50), batch: (100/271), loss: 0.06022061035037041\n",
      "Epoch (42/50), batch: (120/271), loss: 0.062287818640470505\n",
      "Epoch (42/50), batch: (140/271), loss: 0.06344456970691681\n",
      "Epoch (42/50), batch: (160/271), loss: 0.06562598049640656\n",
      "Epoch (42/50), batch: (180/271), loss: 0.06840310990810394\n",
      "Epoch (42/50), batch: (200/271), loss: 0.07031429558992386\n",
      "Epoch (42/50), batch: (220/271), loss: 0.07237636297941208\n",
      "Epoch (42/50), batch: (240/271), loss: 0.07399269193410873\n",
      "Epoch (42/50), batch: (260/271), loss: 0.07582695037126541\n",
      "42/50 loss 0.08\n",
      "Epoch (43/50), batch: (0/271), loss: 0.06576564908027649\n",
      "Epoch (43/50), batch: (20/271), loss: 0.05475116893649101\n",
      "Epoch (43/50), batch: (40/271), loss: 0.05628342181444168\n",
      "Epoch (43/50), batch: (60/271), loss: 0.05892089009284973\n",
      "Epoch (43/50), batch: (80/271), loss: 0.059405531734228134\n",
      "Epoch (43/50), batch: (100/271), loss: 0.06020975112915039\n",
      "Epoch (43/50), batch: (120/271), loss: 0.06195656582713127\n",
      "Epoch (43/50), batch: (140/271), loss: 0.06326251477003098\n",
      "Epoch (43/50), batch: (160/271), loss: 0.06567501276731491\n",
      "Epoch (43/50), batch: (180/271), loss: 0.06682494282722473\n",
      "Epoch (43/50), batch: (200/271), loss: 0.06844275444746017\n",
      "Epoch (43/50), batch: (220/271), loss: 0.06982328742742538\n",
      "Epoch (43/50), batch: (240/271), loss: 0.07170764356851578\n",
      "Epoch (43/50), batch: (260/271), loss: 0.07296538352966309\n",
      "43/50 loss 0.07\n",
      "Epoch (44/50), batch: (0/271), loss: 0.03928970545530319\n",
      "Epoch (44/50), batch: (20/271), loss: 0.046008940786123276\n",
      "Epoch (44/50), batch: (40/271), loss: 0.05058732256293297\n",
      "Epoch (44/50), batch: (60/271), loss: 0.05397895723581314\n",
      "Epoch (44/50), batch: (80/271), loss: 0.05505311116576195\n",
      "Epoch (44/50), batch: (100/271), loss: 0.05573468282818794\n",
      "Epoch (44/50), batch: (120/271), loss: 0.057447124272584915\n",
      "Epoch (44/50), batch: (140/271), loss: 0.05817269906401634\n",
      "Epoch (44/50), batch: (160/271), loss: 0.059743065387010574\n",
      "Epoch (44/50), batch: (180/271), loss: 0.061810750514268875\n",
      "Epoch (44/50), batch: (200/271), loss: 0.06381174921989441\n",
      "Epoch (44/50), batch: (220/271), loss: 0.06636452674865723\n",
      "Epoch (44/50), batch: (240/271), loss: 0.06767567247152328\n",
      "Epoch (44/50), batch: (260/271), loss: 0.06927947700023651\n",
      "44/50 loss 0.07\n",
      "Epoch (45/50), batch: (0/271), loss: 0.05071863532066345\n",
      "Epoch (45/50), batch: (20/271), loss: 0.043428998440504074\n",
      "Epoch (45/50), batch: (40/271), loss: 0.04908280819654465\n",
      "Epoch (45/50), batch: (60/271), loss: 0.050916269421577454\n",
      "Epoch (45/50), batch: (80/271), loss: 0.05311409756541252\n",
      "Epoch (45/50), batch: (100/271), loss: 0.05563395097851753\n",
      "Epoch (45/50), batch: (120/271), loss: 0.05704369395971298\n",
      "Epoch (45/50), batch: (140/271), loss: 0.05855337157845497\n",
      "Epoch (45/50), batch: (160/271), loss: 0.05975564569234848\n",
      "Epoch (45/50), batch: (180/271), loss: 0.06161210685968399\n",
      "Epoch (45/50), batch: (200/271), loss: 0.06322115659713745\n",
      "Epoch (45/50), batch: (220/271), loss: 0.06508122384548187\n",
      "Epoch (45/50), batch: (240/271), loss: 0.06627055257558823\n",
      "Epoch (45/50), batch: (260/271), loss: 0.06837853044271469\n",
      "45/50 loss 0.07\n",
      "Epoch (46/50), batch: (0/271), loss: 0.044529758393764496\n",
      "Epoch (46/50), batch: (20/271), loss: 0.04892920330166817\n",
      "Epoch (46/50), batch: (40/271), loss: 0.0475204698741436\n",
      "Epoch (46/50), batch: (60/271), loss: 0.05152466893196106\n",
      "Epoch (46/50), batch: (80/271), loss: 0.05200778692960739\n",
      "Epoch (46/50), batch: (100/271), loss: 0.05392500385642052\n",
      "Epoch (46/50), batch: (120/271), loss: 0.05459766089916229\n",
      "Epoch (46/50), batch: (140/271), loss: 0.05544615909457207\n",
      "Epoch (46/50), batch: (160/271), loss: 0.05773550271987915\n",
      "Epoch (46/50), batch: (180/271), loss: 0.05968715623021126\n",
      "Epoch (46/50), batch: (200/271), loss: 0.061204273253679276\n",
      "Epoch (46/50), batch: (220/271), loss: 0.06319623440504074\n",
      "Epoch (46/50), batch: (240/271), loss: 0.0647425502538681\n",
      "Epoch (46/50), batch: (260/271), loss: 0.06648850440979004\n",
      "46/50 loss 0.07\n",
      "Epoch (47/50), batch: (0/271), loss: 0.043185509741306305\n",
      "Epoch (47/50), batch: (20/271), loss: 0.04999228194355965\n",
      "Epoch (47/50), batch: (40/271), loss: 0.04797520488500595\n",
      "Epoch (47/50), batch: (60/271), loss: 0.05042197182774544\n",
      "Epoch (47/50), batch: (80/271), loss: 0.05189349502325058\n",
      "Epoch (47/50), batch: (100/271), loss: 0.05377461016178131\n",
      "Epoch (47/50), batch: (120/271), loss: 0.05443898215889931\n",
      "Epoch (47/50), batch: (140/271), loss: 0.0563151054084301\n",
      "Epoch (47/50), batch: (160/271), loss: 0.057479456067085266\n",
      "Epoch (47/50), batch: (180/271), loss: 0.059556301683187485\n",
      "Epoch (47/50), batch: (200/271), loss: 0.060684625059366226\n",
      "Epoch (47/50), batch: (220/271), loss: 0.06233011931180954\n",
      "Epoch (47/50), batch: (240/271), loss: 0.0639956071972847\n",
      "Epoch (47/50), batch: (260/271), loss: 0.06542248278856277\n",
      "47/50 loss 0.07\n",
      "Epoch (48/50), batch: (0/271), loss: 0.06645987927913666\n",
      "Epoch (48/50), batch: (20/271), loss: 0.0413607694208622\n",
      "Epoch (48/50), batch: (40/271), loss: 0.04069908708333969\n",
      "Epoch (48/50), batch: (60/271), loss: 0.041911885142326355\n",
      "Epoch (48/50), batch: (80/271), loss: 0.044823627918958664\n",
      "Epoch (48/50), batch: (100/271), loss: 0.04632946476340294\n",
      "Epoch (48/50), batch: (120/271), loss: 0.04959188029170036\n",
      "Epoch (48/50), batch: (140/271), loss: 0.05188779532909393\n",
      "Epoch (48/50), batch: (160/271), loss: 0.052882347255945206\n",
      "Epoch (48/50), batch: (180/271), loss: 0.054317958652973175\n",
      "Epoch (48/50), batch: (200/271), loss: 0.05609557032585144\n",
      "Epoch (48/50), batch: (220/271), loss: 0.05773121863603592\n",
      "Epoch (48/50), batch: (240/271), loss: 0.05993986502289772\n",
      "Epoch (48/50), batch: (260/271), loss: 0.061850178986787796\n",
      "48/50 loss 0.06\n",
      "Epoch (49/50), batch: (0/271), loss: 0.027866734191775322\n",
      "Epoch (49/50), batch: (20/271), loss: 0.0457393154501915\n",
      "Epoch (49/50), batch: (40/271), loss: 0.04683443531394005\n",
      "Epoch (49/50), batch: (60/271), loss: 0.046644825488328934\n",
      "Epoch (49/50), batch: (80/271), loss: 0.04702819511294365\n",
      "Epoch (49/50), batch: (100/271), loss: 0.04851990565657616\n",
      "Epoch (49/50), batch: (120/271), loss: 0.04983986169099808\n",
      "Epoch (49/50), batch: (140/271), loss: 0.05090717226266861\n",
      "Epoch (49/50), batch: (160/271), loss: 0.05222008377313614\n",
      "Epoch (49/50), batch: (180/271), loss: 0.053743429481983185\n",
      "Epoch (49/50), batch: (200/271), loss: 0.055041827261447906\n",
      "Epoch (49/50), batch: (220/271), loss: 0.056978873908519745\n",
      "Epoch (49/50), batch: (240/271), loss: 0.05869950354099274\n",
      "Epoch (49/50), batch: (260/271), loss: 0.06005869805812836\n",
      "49/50 loss 0.06\n",
      "Epoch (50/50), batch: (0/271), loss: 0.041834067553281784\n",
      "Epoch (50/50), batch: (20/271), loss: 0.04031530022621155\n",
      "Epoch (50/50), batch: (40/271), loss: 0.043117981404066086\n",
      "Epoch (50/50), batch: (60/271), loss: 0.0450969822704792\n",
      "Epoch (50/50), batch: (80/271), loss: 0.04613149166107178\n",
      "Epoch (50/50), batch: (100/271), loss: 0.046882402151823044\n",
      "Epoch (50/50), batch: (120/271), loss: 0.04754094034433365\n",
      "Epoch (50/50), batch: (140/271), loss: 0.04900618642568588\n",
      "Epoch (50/50), batch: (160/271), loss: 0.05046623572707176\n",
      "Epoch (50/50), batch: (180/271), loss: 0.05180542916059494\n",
      "Epoch (50/50), batch: (200/271), loss: 0.052906882017850876\n",
      "Epoch (50/50), batch: (220/271), loss: 0.054641783237457275\n",
      "Epoch (50/50), batch: (240/271), loss: 0.05650624632835388\n",
      "Epoch (50/50), batch: (260/271), loss: 0.058621689677238464\n",
      "50/50 loss 0.06\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for i, (center, context) in enumerate(data):\n",
    "        center, context = center.to(device), context.to(device)\n",
    "\n",
    "        # Reset the gradients of the computational graph\n",
    "        cbow.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        UT_ebar = cbow.forward(context)\n",
    "\n",
    "        # Compute negative log-likelihood loss averaged over the\n",
    "        # mini-batch\n",
    "        loss = ce_loss(UT_ebar, center)\n",
    "\n",
    "        # Backward pass to compute gradients of each parameter\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent step according to the chosen optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            loss_avg = float(total_loss / (i + 1))\n",
    "            print(\n",
    "                f\"Epoch ({epoch}/{EPOCHS}), batch: ({i}/{len(data)}), loss: {loss_avg}\"\n",
    "            )\n",
    "\n",
    "    # Print average loss after each epoch\n",
    "    loss_avg = float(total_loss / len(data))\n",
    "    print(\"{}/{} loss {:.2f}\".format(epoch, EPOCHS, loss_avg))\n",
    "\n",
    "    # Predict if `predict_center_word` is implemented\n",
    "    try:\n",
    "        left_words = [\"le\", \"capitaine\"]\n",
    "        right_words = [\"me\", \"dit\"]\n",
    "        word = predict_center_word(word2vec, *left_words, *right_words)[0]\n",
    "        print(\" \".join(left_words + [word] + right_words))\n",
    "    except NameError:\n",
    "        pass"
   ],
   "id": "62e31ebf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction functions\n",
    "\n",
    "Now that the model is learned we can give it a context it has never seen\n",
    "and see what center word it predicts."
   ],
   "id": "ba7089b9-6fd2-4ad2-b65f-24e2be555c93"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:40:34.352691Z",
     "start_time": "2025-12-03T10:40:34.341028Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_center_word_idx(cbow, *context_words_idx, k=10):\n",
    "    \"\"\"Return k-best center words given indexes of context words.\"\"\"\n",
    "\n",
    "    # Create a fake minibatch containing just one example\n",
    "    fake_minibatch = torch.LongTensor(context_words_idx).unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward propagate through the cbow model\n",
    "    score_center = cbow.forward(fake_minibatch)\n",
    "\n",
    "    # Retrieve top k-best indexes using `torch.topk`\n",
    "    _, best_idxs = torch.topk(score_center, k)\n",
    "    \n",
    "    best_idxs_list = best_idxs[0].cpu().tolist()\n",
    "\n",
    "    # Return actual tokens using `idx2tok`\n",
    "    return [idx2tok[idx] for idx in best_idxs_list]\n",
    "\n",
    "\n",
    "def predict_center_word(cbow, *context_words, k=10):\n",
    "    \"\"\"Return k-best centaer words given context words.\"\"\"\n",
    "\n",
    "    idxs = [tok2idx[tok] for tok in context_words]\n",
    "    return predict_center_word_idx(cbow, *idxs, k=k)"
   ],
   "id": "c5fc4079"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:40:38.680260Z",
     "start_time": "2025-12-03T10:40:38.668813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['farragut',\n 'tint',\n 'nautilus',\n 'vivement',\n 'faite',\n 'canot',\n 'tendre',\n 'stewart',\n 'vers',\n 'partit']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_center_word(cbow, \"vingt\", \"mille\", \"sous\", \"les\")\n",
    "predict_center_word(cbow, \"mille\", \"lieues\", \"les\", \"mers\")\n",
    "predict_center_word(cbow, \"le\", \"capitaine\", \"fut\", \"le\")\n",
    "predict_center_word(cbow, \"le\", \"commandant\", \"fut\", \"le\")"
   ],
   "id": "c0bce679"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the embedding\n",
    "\n",
    "We use the library `gensim` to easily compute most similar words for the\n",
    "embedding we just learned. Use `gensim>=4.0.0`."
   ],
   "id": "85649a5c-1f2e-4628-91c9-6d57b70731de"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:40:40.903944Z",
     "start_time": "2025-12-03T10:40:40.893848Z"
    }
   },
   "outputs": [],
   "source": [
    "m = KeyedVectors(vector_size=EMBEDDING_SIZE)\n",
    "m.add_vectors(idx2tok, cbow.embeddings.weight.detach().cpu().numpy())"
   ],
   "id": "b1c8c82d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test most similar words for, for example “lieues”, “mers”,\n",
    "“professeur”… You can look at `words_decreasing_freq` to test most\n",
    "frequent tokens."
   ],
   "id": "334dd18d-39ab-427c-a368-9c9368276d18"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:40:42.094122Z",
     "start_time": "2025-12-03T10:40:42.066996Z"
    }
   },
   "outputs": [],
   "source": [
    "unique, freq = np.unique(tokens, return_counts=True)\n",
    "idxs = freq.argsort()[::-1]\n",
    "words_decreasing_freq = list(zip(unique[idxs], freq[idxs]))"
   ],
   "id": "18576151"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:40:43.769721Z",
     "start_time": "2025-12-03T10:40:43.700727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Capitaine ---\n",
      "[('82', 0.3960556387901306), ('lancez', 0.3935999870300293), ('mystère', 0.35987669229507446), ('canadien', 0.34933871030807495), ('taux', 0.34639009833335876), ('oppose', 0.33605027198791504), ('commencement', 0.3342539966106415), ('indifféremment', 0.3316226899623871), ('tropique', 0.32994717359542847), ('24', 0.3298708498477936)]\n",
      "\n",
      "--- Nemo ---\n",
      "[('bougea', 0.5895227789878845), ('acharnement', 0.4862517714500427), ('regagnâmes', 0.479692667722702), ('souriant', 0.4608738422393799), ('prévenus', 0.42575061321258545), ('nicolas', 0.3940410614013672), ('illuminer', 0.39238157868385315), ('savent', 0.3870413899421692), ('amabilité', 0.3739234507083893), ('tabac', 0.37359553575515747)]\n",
      "\n",
      "--- Nautilus ---\n",
      "[('coulant', 0.3906456232070923), ('tropique', 0.38268303871154785), ('correspondant', 0.3807491958141327), ('pereire', 0.37972718477249146), ('examina', 0.3782614469528198), ('paramatta', 0.37448400259017944), ('milliard', 0.3620046079158783), ('tabac', 0.354568749666214), ('volontairement', 0.3519739806652069), ('gardé', 0.3507424592971802)]\n",
      "\n",
      "--- Mer ---\n",
      "[('louisiade', 0.4179486334323883), ('fortune', 0.4079296290874481), ('nichée', 0.3843521773815155), ('gamme', 0.3829491436481476), ('teintée', 0.37413376569747925), ('guadeloupe', 0.36773547530174255), ('cruauté', 0.3631107807159424), ('conclusion', 0.3538532257080078), ('lard', 0.3519189655780792), ('martinique', 0.3513159453868866)]\n",
      "\n",
      "--- Eau ---\n",
      "[('sang', 0.36449965834617615), ('introduits', 0.34678715467453003), ('backuysen', 0.32094064354896545), ('appellation', 0.3167800009250641), ('entassement', 0.3152981996536255), ('sortira', 0.3098439574241638), ('fuligineuses', 0.3094145357608795), ('pompe', 0.3085857331752777), ('contrastant', 0.3047361969947815), ('roulé', 0.2980736494064331)]\n",
      "\n",
      "--- Lieues ---\n",
      "[('milles', 0.4439861476421356), ('kilomètres', 0.3935973048210144), ('mètres', 0.3900429904460907), ('francs', 0.38368019461631775), ('gibraltar', 0.3486047685146332), ('disposées', 0.3433878421783447), ('lesseps', 0.33124691247940063), ('refusaient', 0.32005712389945984), ('loin', 0.3196747899055481), ('mozambique', 0.31755703687667847)]\n",
      "\n",
      "--- Mètres ---\n",
      "[('milles', 0.5146306753158569), ('pieds', 0.4146125316619873), ('lieues', 0.3900429606437683), ('kilomètres', 0.3781995177268982), ('bonds', 0.362035870552063), ('manœuvrées', 0.3571107089519501), ('toises', 0.3555178940296173), ('favorisent', 0.3470088839530945), ('aricies', 0.3335545063018799), ('adjurèrent', 0.32272353768348694)]\n"
     ]
    }
   ],
   "source": [
    "# Personnages et entités clés\n",
    "print(\"--- Capitaine ---\")\n",
    "print(m.most_similar(\"capitaine\"))\n",
    "\n",
    "print(\"\\n--- Nemo ---\")\n",
    "print(m.most_similar(\"nemo\"))\n",
    "\n",
    "print(\"\\n--- Nautilus ---\")\n",
    "print(m.most_similar(\"nautilus\"))\n",
    "\n",
    "# Environnement\n",
    "print(\"\\n--- Mer ---\")\n",
    "print(m.most_similar(\"mer\"))\n",
    "\n",
    "print(\"\\n--- Eau ---\")\n",
    "print(m.most_similar(\"eau\"))\n",
    "\n",
    "# Unités de mesure (très fréquentes chez Jules Verne)\n",
    "print(\"\\n--- Lieues ---\")\n",
    "print(m.most_similar(\"lieues\"))\n",
    "\n",
    "print(\"\\n--- Mètres ---\")\n",
    "print(m.most_similar(\"mètres\"))"
   ],
   "id": "621687ee"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "path": "/home/sylvain/.local/share/jupyter/kernels/python3"
  }
 }
}
