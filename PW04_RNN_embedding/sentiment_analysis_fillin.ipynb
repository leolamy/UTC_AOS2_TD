{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and RNN for sentiment analysis\n",
    "\n",
    "The goal of the following notebook is to predict whether a written\n",
    "critic about a movie is positive or negative. For that we will try three\n",
    "models. A simple linear model on the word embeddings, a recurrent neural\n",
    "network and a CNN.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Libraries and Imports\n",
    "\n",
    "First some imports are needed."
   ],
   "id": "af221498-e941-4428-bfd2-59f7ef5420a1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:43:29.631689Z",
     "start_time": "2025-12-03T10:43:22.485990Z"
    }
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers"
   ],
   "id": "b1cdd4f6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables\n",
    "\n",
    "First let’s define a few variables. `EMBEDDING_DIM` is the dimension of\n",
    "the vector space used to embed all the words of the vocabulary.\n",
    "`SEQ_LENGTH` is the maximum length of a sequence, `BATCH_SIZE` is the\n",
    "size of the batches used in stochastic optimization algorithms and\n",
    "`NUM_EPOCHS` the number of times we are going thought the entire\n",
    "training set during the training phase."
   ],
   "id": "4c36a5b0-a957-4b71-b82c-d498390baf3f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:44:35.005313Z",
     "start_time": "2025-12-03T10:44:34.992002Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 8\n",
    "SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "40b1c37a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `IMDb` dataset\n",
    "\n",
    "We use the `datasets` library to load the `IMDb` dataset."
   ],
   "id": "c176c916-bc61-4382-ba13-67651770328b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:44:46.125466Z",
     "start_time": "2025-12-03T10:44:38.285574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "README.md: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e74cf432e99f4292b135df4b768a3e26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b6a0ad2f90e41f499752d04eb00b1b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23a4ef10d27141f89cfacdbb80ac0afd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8ce354e5c83468a8b8c91af21e95032"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4a8b2a51d9143e080c3c874bf10f726"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac3f28df584b42df93739c4141089461"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97127eeb2b524f1aaaac3bd5b8bd00d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']\n",
    "\n",
    "train_set[0]\n",
    "\n",
    "print(f\"Number of training examples: {len(train_set)}\")\n",
    "print(f\"Number of testing examples: {len(test_set)}\")"
   ],
   "id": "7934ce80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a vocabulary out of `IMDb` from a tokenizer\n",
    "\n",
    "We first need a tokenizer that takes a text a returns a list of tokens.\n",
    "There are many tokenizers available from other libraries. Here we use\n",
    "the `tokenizers` library."
   ],
   "id": "0b130e41-531c-4389-9f3a-86195ec82fc0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:45:22.161997Z",
     "start_time": "2025-12-03T10:45:22.155723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use a word-level tokenizer in lower case\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = normalizers.Lowercase()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ],
   "id": "218d932c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to define the set of words that will be understood by the\n",
    "model: this is the vocabulary. We build it from the training set."
   ],
   "id": "b612e77e-8d85-44e7-b613-ca3bc2fab800"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:52:41.128083Z",
     "start_time": "2025-12-03T10:52:40.124819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "4995"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts = train_set['text']\n",
    "test_texts = test_set['text']\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=10000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "tokenizer.train_from_iterator(train_texts, trainer)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "UNK_IDX, PAD_IDX = vocab[\"[UNK]\"], vocab[\"[PAD]\"]\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "tokenizer.encode(\"All your base are belong to us\").tokens\n",
    "tokenizer.encode(\"All your base are belong to us\").ids\n",
    "\n",
    "vocab['plenty']\n",
    "vocab['belong']"
   ],
   "id": "28ee601b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "The training loop is decomposed into 3 different functions:\n",
    "\n",
    "-   `train_epoch`\n",
    "-   `evaluate`\n",
    "-   `train`\n",
    "\n",
    "### Collate function\n",
    "\n",
    "The collate function maps raw samples coming from the dataset to padded\n",
    "tensors of numericalized tokens ready to be fed to the model."
   ],
   "id": "d897cee6-476d-4407-babc-5fd893b42d02"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T11:13:55.118324Z",
     "start_time": "2025-12-03T11:13:55.116543Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    def collate(text):\n",
    "        \"\"\"Turn a text into a tensor of integers.\"\"\"\n",
    "        ids = tokenizer.encode(text).ids[:SEQ_LENGTH]\n",
    "        return torch.LongTensor(ids)\n",
    "\n",
    "    src_batch = [collate(sample[\"text\"]) for sample in batch]\n",
    "\n",
    "    # Pad list of tensors using `pad_sequence`\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    # Define the labels tensor\n",
    "    tgt_batch = torch.Tensor([sample[\"label\"] for sample in batch])\n",
    "\n",
    "    return src_batch, tgt_batch"
   ],
   "id": "e9f41d2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `accuracy` function\n",
    "\n",
    "We need to implement an accuracy function to be used in the\n",
    "`train_epoch` function (see below)."
   ],
   "id": "bb78473a-e31b-47f5-b5f8-d716d86a8e40"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T11:58:34.691020Z",
     "start_time": "2025-12-03T11:58:34.671562Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # `predictions` and `labels` are both tensors of same length\n",
    "\n",
    "    # Implement accuracy\n",
    "    return torch.sum((torch.sigmoid(predictions) > 0.5).float() == (labels > 0.5)).item() / len(predictions)\n",
    "\n",
    "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
    "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3"
   ],
   "id": "3372c02c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `train_epoch` function"
   ],
   "id": "a1bb4b73-a3e9-4bed-97c6-f966d501b190"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:05:13.657100Z",
     "start_time": "2025-12-03T12:05:13.655853Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, optimizer: Optimizer):\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True\n",
    "    )\n",
    "\n",
    "    matches = 0\n",
    "    losses = 0\n",
    "    for sequences, labels in train_dataloader:\n",
    "        sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Implement a step of the algorithm:\n",
    "        #\n",
    "        # - set gradients to zero\n",
    "        # - forward propagate examples in `batch`\n",
    "        # - compute `loss` with chosen criterion\n",
    "        # - back-propagate gradients\n",
    "        # - gradient step\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        acc = accuracy(predictions, labels)\n",
    "\n",
    "        matches += len(predictions) * acc\n",
    "\n",
    "    return losses / len(train_set), matches / len(train_set)"
   ],
   "id": "d961e906"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `evaluate` function"
   ],
   "id": "37baabc9-d96e-43f6-a93e-fbf42b26fd2d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:05:23.978889Z",
     "start_time": "2025-12-03T12:05:23.966053Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    losses = 0\n",
    "    matches = 0\n",
    "    for sequences, labels in val_dataloader:\n",
    "        sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        acc = accuracy(predictions, labels)\n",
    "        matches += len(predictions) * acc\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(test_set), matches / len(test_set)"
   ],
   "id": "a037db8c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `train` function"
   ],
   "id": "660f4cff-6cb9-4e28-99b6-39af1ec292a7"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:05:33.774088Z",
     "start_time": "2025-12-03T12:05:33.759603Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = timer()\n",
    "        train_loss, train_acc = train_epoch(model, optimizer)\n",
    "        end_time = timer()\n",
    "        val_loss, val_acc = evaluate(model)\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"Train loss: {train_loss:.3f}, \"\n",
    "            f\"Train acc: {train_acc:.3f}, \"\n",
    "            f\"Val loss: {val_loss:.3f}, \"\n",
    "            f\"Val acc: {val_acc:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )"
   ],
   "id": "f1ac3bd3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to predict from a character string"
   ],
   "id": "f2c57de6-3fa3-4a1a-afc4-c9d3047861cd"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:05:44.916682Z",
     "start_time": "2025-12-03T12:05:44.901186Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"Predict sentiment of given sentence according to model\"\n",
    "\n",
    "    tensor, _ = collate_fn([{\"label\": 0, \"text\": sentence}])\n",
    "    model.to(DEVICE)\n",
    "    tensor = tensor.to(DEVICE)\n",
    "    prediction = model(tensor)\n",
    "    pred = torch.sigmoid(prediction)\n",
    "    return pred.item()"
   ],
   "id": "db934ca4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Training a linear classifier with an embedding\n",
    "\n",
    "We first test a simple linear classifier on the word embeddings."
   ],
   "id": "16da4875-79e0-4ed9-9365-5d59a5c066dc"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:55:10.490475Z",
     "start_time": "2025-12-03T15:55:10.482534Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Define an embedding of `vocab_size` words into a vector space\n",
    "        # of dimension `embedding_dim`.\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # Define a linear layer from dimension `seq_length` *\n",
    "        # `embedding_dim` to 1.\n",
    "        self.l1 = nn.Linear(self.seq_length*self.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size `seq_length` * `batch_size`\n",
    "\n",
    "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
    "        # of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Flatten the embedded words and feed it to the linear layer. `flatten`\n",
    "        # must be of size `batch_size` * (`seq_length` * `embedding_dim`). You\n",
    "        # might need to use `permute` first.\n",
    "        flatten = embedded.permute((1,0,2)).reshape(-1, self.seq_length*self.embedding_dim)\n",
    "\n",
    "        # Apply the linear layer and return a squeezed version\n",
    "        # `l1` is of size `batch_size`\n",
    "        return self.l1(flatten).squeeze()"
   ],
   "id": "49b37996"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:56:42.324410Z",
     "start_time": "2025-12-03T15:55:15.763744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80513\n",
      "80513\n",
      "Epoch: 1, Train loss: 0.000, Train acc: 0.502, Val loss: 0.001, Val acc: 0.515, Epoch time = 4.703s\n",
      "Epoch: 2, Train loss: 0.000, Train acc: 0.538, Val loss: 0.001, Val acc: 0.528, Epoch time = 4.548s\n",
      "Epoch: 3, Train loss: 0.000, Train acc: 0.563, Val loss: 0.001, Val acc: 0.541, Epoch time = 4.316s\n",
      "Epoch: 4, Train loss: 0.000, Train acc: 0.584, Val loss: 0.001, Val acc: 0.555, Epoch time = 4.354s\n",
      "Epoch: 5, Train loss: 0.000, Train acc: 0.599, Val loss: 0.001, Val acc: 0.567, Epoch time = 4.272s\n",
      "Epoch: 6, Train loss: 0.000, Train acc: 0.617, Val loss: 0.001, Val acc: 0.582, Epoch time = 4.220s\n",
      "Epoch: 7, Train loss: 0.000, Train acc: 0.633, Val loss: 0.001, Val acc: 0.597, Epoch time = 4.220s\n",
      "Epoch: 8, Train loss: 0.000, Train acc: 0.649, Val loss: 0.001, Val acc: 0.615, Epoch time = 4.307s\n",
      "Epoch: 9, Train loss: 0.000, Train acc: 0.665, Val loss: 0.001, Val acc: 0.632, Epoch time = 4.275s\n",
      "Epoch: 10, Train loss: 0.000, Train acc: 0.681, Val loss: 0.001, Val acc: 0.648, Epoch time = 4.217s\n"
     ]
    }
   ],
   "source": [
    "embedding_net = EmbeddingNet(VOCAB_SIZE, EMBEDDING_DIM, SEQ_LENGTH)\n",
    "print(sum(torch.numel(e) for e in embedding_net.parameters() if e.requires_grad))\n",
    "\n",
    "print(\n",
    "    VOCAB_SIZE * EMBEDDING_DIM + # Embeddings\n",
    "    (SEQ_LENGTH * EMBEDDING_DIM + 1) # Linear\n",
    ")\n",
    "\n",
    "optimizer = Adam(embedding_net.parameters())\n",
    "train(embedding_net, optimizer)"
   ],
   "id": "999cef91"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a linear classifier with a pretrained embedding\n",
    "\n",
    "Load a GloVe pretrained embedding instead"
   ],
   "id": "9f9ca5d7-7e8b-452c-b0c8-0ae2a8914bf8"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:07:09.221699Z",
     "start_time": "2025-12-03T16:06:46.903490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download GloVe word embedding\n",
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "unknown_vector = glove_vectors.get_mean_vector(glove_vectors.index_to_key)\n",
    "vocab_vectors = torch.tensor(np.stack([glove_vectors[e] if e in glove_vectors else unknown_vector for e in vocab.keys()]))\n",
    "\n",
    "class GloVeEmbeddingNet(nn.Module):\n",
    "    def __init__(self, seq_length, vocab_vectors, freeze=True):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
    "        self.embedding_dim = vocab_vectors.size(1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Same forward as in `EmbeddingNet`\n",
    "        # `x` is of size `batch_size` * `seq_length`\n",
    "        embedded = self.embedding(x)\n",
    "        flatten = embedded.permute((1,0,2)).reshape(-1, self.seq_length*self.embedding_dim)\n",
    "\n",
    "        # L1 is of size batch_size\n",
    "        return self.l1(flatten).squeeze()"
   ],
   "id": "33d7d8cf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pretrained embedding without fine-tuning"
   ],
   "id": "4686d8c2-4dcd-4fc2-9330-22c131bc17ec"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:09:00.590585Z",
     "start_time": "2025-12-03T16:07:34.623227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1601\n",
      "1601\n",
      "Epoch: 1, Train loss: 0.000, Train acc: 0.520, Val loss: 0.001, Val acc: 0.528, Epoch time = 4.574s\n",
      "Epoch: 2, Train loss: 0.000, Train acc: 0.568, Val loss: 0.001, Val acc: 0.539, Epoch time = 4.576s\n",
      "Epoch: 3, Train loss: 0.000, Train acc: 0.586, Val loss: 0.001, Val acc: 0.540, Epoch time = 4.272s\n",
      "Epoch: 4, Train loss: 0.000, Train acc: 0.595, Val loss: 0.001, Val acc: 0.540, Epoch time = 4.366s\n",
      "Epoch: 5, Train loss: 0.000, Train acc: 0.603, Val loss: 0.001, Val acc: 0.544, Epoch time = 4.626s\n",
      "Epoch: 6, Train loss: 0.000, Train acc: 0.607, Val loss: 0.001, Val acc: 0.544, Epoch time = 4.287s\n",
      "Epoch: 7, Train loss: 0.000, Train acc: 0.606, Val loss: 0.001, Val acc: 0.541, Epoch time = 4.192s\n",
      "Epoch: 8, Train loss: 0.000, Train acc: 0.610, Val loss: 0.001, Val acc: 0.547, Epoch time = 4.347s\n",
      "Epoch: 9, Train loss: 0.000, Train acc: 0.609, Val loss: 0.001, Val acc: 0.543, Epoch time = 4.232s\n",
      "Epoch: 10, Train loss: 0.000, Train acc: 0.609, Val loss: 0.001, Val acc: 0.544, Epoch time = 4.191s\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_net_freeze = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
    "print(sum(torch.numel(e) for e in glove_embedding_net_freeze.parameters() if e.requires_grad))\n",
    "\n",
    "print(\n",
    "    (SEQ_LENGTH * 25 + 1) # Linear\n",
    ")\n",
    "\n",
    "optimizer = Adam(glove_embedding_net_freeze.parameters())\n",
    "train(glove_embedding_net_freeze, optimizer)"
   ],
   "id": "67da3447"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the pretrained embedding"
   ],
   "id": "be6d17e0-51d2-4d6a-9241-3c18d4ab209c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:09:17.369207Z",
     "start_time": "2025-12-03T16:09:17.363161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model and don't freeze embedding weights\n",
    "glove_embedding_net = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=False)"
   ],
   "id": "08f9f2ba"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural network with frozen pretrained embedding"
   ],
   "id": "0620953b-36be-4f11-b1a6-ae13fe68a2d6"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:18:54.270647Z",
     "start_time": "2025-12-03T16:16:16.263033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38201\n",
      "38201\n",
      "Epoch: 1, Train loss: 0.000, Train acc: 0.509, Val loss: 0.001, Val acc: 0.518, Epoch time = 8.967s\n",
      "Epoch: 2, Train loss: 0.000, Train acc: 0.535, Val loss: 0.001, Val acc: 0.523, Epoch time = 9.848s\n",
      "Epoch: 3, Train loss: 0.000, Train acc: 0.582, Val loss: 0.001, Val acc: 0.582, Epoch time = 8.665s\n",
      "Epoch: 4, Train loss: 0.000, Train acc: 0.605, Val loss: 0.001, Val acc: 0.601, Epoch time = 9.179s\n",
      "Epoch: 5, Train loss: 0.000, Train acc: 0.636, Val loss: 0.001, Val acc: 0.627, Epoch time = 9.972s\n",
      "Epoch: 6, Train loss: 0.000, Train acc: 0.663, Val loss: 0.001, Val acc: 0.642, Epoch time = 8.732s\n",
      "Epoch: 7, Train loss: 0.000, Train acc: 0.700, Val loss: 0.001, Val acc: 0.651, Epoch time = 8.989s\n",
      "Epoch: 8, Train loss: 0.000, Train acc: 0.729, Val loss: 0.001, Val acc: 0.655, Epoch time = 9.678s\n",
      "Epoch: 9, Train loss: 0.000, Train acc: 0.763, Val loss: 0.001, Val acc: 0.662, Epoch time = 9.179s\n",
      "Epoch: 10, Train loss: 0.000, Train acc: 0.791, Val loss: 0.001, Val acc: 0.647, Epoch time = 9.689s\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_vectors, freeze=True):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Define pretrained embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "\n",
    "        # Size of input `x_t` from `embedding`\n",
    "        self.embedding_size = self.embedding.embedding_dim\n",
    "        self.input_size = self.embedding_size\n",
    "\n",
    "        # Size of hidden state `h_t`\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define a GRU\n",
    "        self.gru = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size)\n",
    "\n",
    "        # Linear layer on last hidden state\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # `x` is of size `seq_length` * `batch_size` and `h0` is of size 1\n",
    "        # * `batch_size` * `hidden_size`\n",
    "\n",
    "        # Define first hidden state in not provided\n",
    "        # h0 = mémoire du modèle vide\n",
    "        if h0 is None:\n",
    "            # Get batch and define `h0` which is of size 1 * `batch_size` *\n",
    "            # `hidden_size`\n",
    "            batch_size = x.size(1)\n",
    "            h0 = torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n",
    "\n",
    "        # `embedded` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Define `output` and `hidden` returned by GRU:\n",
    "        #\n",
    "        # - `output` is of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        #   and gathers all the hidden states along the sequence.\n",
    "        # - `hidden` is of size 1 * `batch_size` * `embedding_dim` and is the\n",
    "        #   last hidden state.\n",
    "        output, hidden = self.gru(embedded, h0)\n",
    "\n",
    "        # Apply a linear layer on the last hidden state to have a score tensor\n",
    "        # of size 1 * `batch_size` * 1, and return a one-dimensional tensor of\n",
    "        # size `batch_size`.\n",
    "        return self.linear(hidden).squeeze()\n",
    "\n",
    "\n",
    "rnn = RNN(hidden_size=100, vocab_vectors=vocab_vectors)\n",
    "print(sum(torch.numel(e) for e in rnn.parameters() if e.requires_grad))\n",
    "\n",
    "hidden_size = 100\n",
    "print(\n",
    "    3 * hidden_size * (hidden_size + 25 + 2) + # GRU (2 bias vectors instead of 1)\n",
    "    hidden_size + 1 # Linear\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.005)\n",
    "train(rnn, optimizer)"
   ],
   "id": "59420d97"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "path": "/home/sylvain/.local/share/jupyter/kernels/python3"
  }
 }
}
