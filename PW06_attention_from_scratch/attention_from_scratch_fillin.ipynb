{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The transformer architecture from scratch"
   ],
   "id": "8bfec1c8-ee30-4594-b406-21b93c9ce057"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:30:09.389660Z",
     "start_time": "2025-12-10T13:30:08.429302Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Iterable\n",
    "from timeit import default_timer as timer\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "id": "block-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy dataset"
   ],
   "id": "67cd1077-9585-4c7f-918e-452373b6c7c4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:30:25.025343Z",
     "start_time": "2025-12-10T13:30:25.017101Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate_deterministic(input_sequence):\n",
    "    target_sequence = []\n",
    "    for i, elt in enumerate(input_sequence):\n",
    "        try:\n",
    "            offset = int(elt)\n",
    "        except ValueError:  # It is a letter\n",
    "            target_sequence.append(elt)\n",
    "        else:               # Special token, do the lookup\n",
    "            if i + offset < 0 or i + offset > len(input_sequence) - 1:\n",
    "                pass\n",
    "            else:\n",
    "                k = min(max(0, i + offset), len(input_sequence) - 1)\n",
    "                target_sequence.append(input_sequence[k])\n",
    "\n",
    "    return target_sequence\n",
    "\n",
    "\n",
    "class GotoDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed=None,\n",
    "        n_sequences=100,\n",
    "        min_length=4,\n",
    "        max_length=20,\n",
    "        n_letters=3,\n",
    "        offsets=[4, 5, 6],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        full_vocab = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        full_vocab = list(full_vocab.upper()) + list(full_vocab)\n",
    "        assert(n_letters <= len(full_vocab))\n",
    "\n",
    "        self.vocab = np.array(\n",
    "            [s + str(d) for s in [\"+\", \"-\"] for d in offsets] + full_vocab[:n_letters]\n",
    "        )\n",
    "        self.n_tokens = len(self.vocab)\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.seed = seed\n",
    "        self.n_sequences = n_sequences\n",
    "\n",
    "        # Dataset generation\n",
    "        rs = np.random.RandomState(self.seed)\n",
    "        seq_lengths = rs.randint(\n",
    "            self.min_length, self.max_length, size=self.n_sequences\n",
    "        )\n",
    "        self.input_sequences = [\n",
    "            list(self.vocab[rs.randint(self.n_tokens, size=seq_length)])\n",
    "            for seq_length in seq_lengths\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_sequences\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        input_sequence = self.input_sequences[i]\n",
    "        target_sequence = translate_deterministic(input_sequence)\n",
    "        return input_sequence, target_sequence"
   ],
   "id": "block-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ],
   "id": "9aa9b6d9-ab33-4d7b-991c-de8c99b6f040"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:30:27.446819Z",
     "start_time": "2025-12-10T13:30:27.364995Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = GotoDataset()\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "idx2tok = special_tokens + dataset.vocab.tolist()\n",
    "tok2idx = {token: i for i, token in enumerate(idx2tok)}\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = [tok2idx[tok] for tok in special_tokens]"
   ],
   "id": "block-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate function"
   ],
   "id": "4f2f65b0-86ac-46cd-8e41-5b79fe947476"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "\n",
    "        # Numericalize list of tokens using `vocab`.\n",
    "        #\n",
    "        # - Don't forget to add beginning of sequence and end of sequence tokens\n",
    "        #   before numericalizing.\n",
    "        #\n",
    "        # - Use `torch.LongTensor` instead of `torch.Tensor` because the next\n",
    "        #   step is an embedding that needs integers for its lookup table.\n",
    "        # <answer>\n",
    "        src_tensor = torch.LongTensor([tok2idx[tok] for tok in [\"<bos>\"] + src_sample + [\"<eos>\"]])\n",
    "        tgt_tensor = torch.LongTensor([tok2idx[tok] for tok in [\"<bos>\"] + tgt_sample + [\"<eos>\"]])\n",
    "        # </answer>\n",
    "\n",
    "        # Append numericalized sequence to `src_batch` and `tgt_batch`\n",
    "        src_batch.append(src_tensor)\n",
    "        tgt_batch.append(tgt_tensor)\n",
    "\n",
    "    # Turn `src_batch` and `tgt_batch` that are lists of 1-dimensional\n",
    "    # tensors of varying sizes into tensors with same size with\n",
    "    # padding. Use `pad_sequence` with padding value to do so.\n",
    "    #\n",
    "    # Important notice: by default resulting tensors are of size\n",
    "    # `max_seq_length` * `batch_size`; the mini-batch size is on the\n",
    "    # *second dimension*.\n",
    "    # <answer>\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    # </answer>\n",
    "\n",
    "    return src_batch, tgt_batch"
   ],
   "id": "block-4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters of transformer model"
   ],
   "id": "b4c7e77f-c0b8-4cbf-8dba-67344cdafdef"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:30:45.498526Z",
     "start_time": "2025-12-10T13:30:45.491436Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Size of source and target vocabulary\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "\n",
    "# Number of sequences generated for the training set\n",
    "N_SEQUENCES = 7000\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Size of embeddings\n",
    "EMB_SIZE = 64\n",
    "\n",
    "# Number of heads for the multihead attention\n",
    "NHEAD = 1\n",
    "\n",
    "# Size of hidden layer of FFN\n",
    "FFN_HID_DIM = 128\n",
    "\n",
    "# Size of mini-batches\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Number of stacked encoder modules\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "\n",
    "# Number of stacked decoder modules\n",
    "NUM_DECODER_LAYERS = 1"
   ],
   "id": "block-5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer encoder"
   ],
   "id": "2b4d2e71-be5f-400a-95fb-b9eff040e034"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:30:48.930949Z",
     "start_time": "2025-12-10T13:30:48.923088Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float = 0.1, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Define Tk/2pi for even k between 0 and `emb_size`. Use\n",
    "        # `torch.arange`.\n",
    "        # <answer>\n",
    "        Tk_over_2pi = 10000 ** (torch.arange(0, emb_size, 2) / emb_size)\n",
    "        # </answer>\n",
    "\n",
    "        # Define `t = 0, 1,..., maxlen-1`. Use `torch.arange`.\n",
    "        # <answer>\n",
    "        t = torch.arange(maxlen)\n",
    "        # </answer>\n",
    "\n",
    "        # Outer product between `t` and `1/Tk_over_2pi` to have a\n",
    "        # matrix of size `maxlen` * `emb_size // 2`. Use\n",
    "        # `torch.outer`.\n",
    "        # <answer>\n",
    "        outer = torch.outer(t, 1 / Tk_over_2pi)\n",
    "        # </answer>\n",
    "\n",
    "        pos_embedding = torch.empty((maxlen, emb_size))\n",
    "\n",
    "        # Fill `pos_embedding` with either sine or cosine of `outer`.\n",
    "        # <answer>\n",
    "        pos_embedding[:, 0::2] = torch.sin(outer)\n",
    "        pos_embedding[:, 1::2] = torch.cos(outer)\n",
    "        # </answer>\n",
    "\n",
    "        # Add fake mini-batch dimension to be able to use broadcasting\n",
    "        # in `forward` method.\n",
    "        pos_embedding = pos_embedding.unsqueeze(1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Save `pos_embedding` when serializing the model even if it is not a\n",
    "        # set of parameters\n",
    "        self.register_buffer(\"pos_embedding\", pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        # `token_embedding` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_size`. Use broadcasting to add the positional embedding\n",
    "        # that is of size `seq_length` * 1 * `embedding_size`.\n",
    "        # <answer>\n",
    "        seq_length = token_embedding.size(0)\n",
    "        positional_encoding = token_embedding + self.pos_embedding[:seq_length, :]\n",
    "        # </answer>\n",
    "\n",
    "        return self.dropout(positional_encoding)"
   ],
   "id": "block-6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T14:59:22.465996Z",
     "start_time": "2025-12-10T14:59:22.463017Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            p=None,                  # Embedding size of input tokens\n",
    "            d_ff=None,               # Size of hidden layer in MLP\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Size of embedding. Here sizes of embedding, keys, queries\n",
    "        # and values are the same.\n",
    "        self.p = p\n",
    "        d_q = d_v = d_k = p\n",
    "\n",
    "        # Size of hidden layer in MLP\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Compute query, key and value from input\n",
    "        self.enc_Q = nn.Linear(p, d_q)\n",
    "        self.enc_K = nn.Linear(p, d_k)\n",
    "        self.enc_V = nn.Linear(p, d_v)\n",
    "\n",
    "        # Linear transform just before first residual mapping\n",
    "        self.enc_W0 = nn.Linear(d_v, p)\n",
    "\n",
    "        # Layer normalization after first residual mapping\n",
    "        self.enc_ln1 = nn.LayerNorm(p)\n",
    "\n",
    "        # Position-wise MLP\n",
    "        self.enc_W1 = nn.Linear(p, d_ff)\n",
    "        self.enc_W2 = nn.Linear(d_ff, p)\n",
    "\n",
    "        # Final layer normalization of second residual mapping\n",
    "        self.enc_ln2 = nn.LayerNorm(p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation in encoder. Input tensor `X` is of size\n",
    "        # `seq_length` * `batch_size` * `p`.\n",
    "\n",
    "        # Query, key and value of the encoder. Use `enc_Q`, `enc_K`\n",
    "        # and `enc_V`.\n",
    "        Q = self.enc_Q(X)\n",
    "        K = self.enc_K(X)\n",
    "        V = self.enc_V(X)\n",
    "\n",
    "        # Score attention from `Q` and `K`. We need to compute `QK^T` but both\n",
    "        # `Q` and `K` are not just simple matrices but batch of matrices. Both\n",
    "        # `Q` and `K` are in fact of size `seq_length` * `batch_size` *\n",
    "        # `emb_size`. Two ways to compute the batched matrix product:\n",
    "        #\n",
    "        # - permute dimensions using `torch.permute` so that `batch_size` is the\n",
    "        #   first dimension and use `torch.bmm` that will perform the batch\n",
    "        #   matrix product with respect to the first dimension,\n",
    "        # - use `torch.einsum` to specify the product.\n",
    "        S = torch.bmm(Q.permute([1,0,2]), K.permute([1,2,0])) / math.sqrt(self.p)\n",
    "        #S = torch.einsum('sbe, Sbe -> bsS', Q, K) / math.sqrt(self.p) s seq_length Q S seq_length K\n",
    " \n",
    "        # Compute attention from `S` and `V`. You can use `F.softmax` with `dim`\n",
    "        # argument. Since the mini-batch dimension is now the first one for `S`\n",
    "        # we can use `torch.bmm` with `S` (after softmax). That is not the case\n",
    "        # for `V` so we need to transpose it first. Don't forget to transpose\n",
    "        # again after the product to have a matrix `seq_length` * `batch_size` *\n",
    "        # `emb_size` compatible with `X` for the residual mapping.\n",
    "        A = F.softmax(S, dim=2) # on fait le softmax sur les clés (dernière position) car QK, si on enleve minibatch -> dernière colonne\n",
    "        T = torch.einsum('bsS, Sbe -> sbe', A, V)\n",
    "\n",
    "        # First residual mapping and layer normalization\n",
    "        U = self.enc_ln1(self.enc_W0(T) + X)\n",
    "\n",
    "        # FFN on each token\n",
    "        Z = self.enc_W2(F.relu(self.enc_W1(U)))\n",
    "\n",
    "        # Second residual mapping and layer normalization\n",
    "        Xp = self.enc_ln2(Z+U)\n",
    "\n",
    "        return Xp"
   ],
   "id": "block-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer decoder"
   ],
   "id": "4dd0d388-08e6-4776-8dfb-841ff7a4ebf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            p=None,                  # Embedding size of input tokens\n",
    "            d_ff=None,               # Size of hidden layer in MLP\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Size of embedding. Here, sizes of embedding, keys, queries\n",
    "        # and values are the same.\n",
    "        self.p = p\n",
    "        self.d_q = self.d_v = self.d_k = p\n",
    "\n",
    "        # Size of hidden layer in MLP\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Compute query, key and value from input\n",
    "        self.dec_Q1 = nn.Linear(p, self.d_q)\n",
    "        self.dec_K1 = nn.Linear(p, self.d_k)\n",
    "        self.dec_V1 = nn.Linear(p, self.d_v)\n",
    "\n",
    "        # Linear transform just before first residual mapping\n",
    "        self.dec_W0 = nn.Linear(self.d_v, p)\n",
    "\n",
    "        # Layer normalization after first residual mapping\n",
    "        self.dec_ln1 = nn.LayerNorm(p)\n",
    "\n",
    "        # Key-value cross-attention\n",
    "        self.dec_Q2 = nn.Linear(p, self.d_k)\n",
    "        self.dec_K2 = nn.Linear(p, self.d_k)\n",
    "        self.dec_V2 = nn.Linear(p, self.d_v)\n",
    "\n",
    "        # Linear transform just before first residual mapping\n",
    "        self.dec_W1 = nn.Linear(self.d_v, p)\n",
    "\n",
    "        # Layer normalization after second residual mapping\n",
    "        self.dec_ln2 = nn.LayerNorm(p)\n",
    "\n",
    "        # Position-wise MLP\n",
    "        self.dec_W2 = nn.Linear(p, d_ff)\n",
    "        self.dec_W3 = nn.Linear(d_ff, p)\n",
    "\n",
    "        # Final layer normalization of second residual mapping\n",
    "        self.dec_ln3 = nn.LayerNorm(p)\n",
    "\n",
    "    def forward(self, Xp, Y):\n",
    "        # Forward propagation in decoder. Input tensor `Xp` is of size\n",
    "        # `seq_length_src` * `batch_size` * `p` and `Y` is of size\n",
    "        # `seq_length_tgt` * `batch_size` * `p`.\n",
    "\n",
    "\n",
    "        # Set number of tokens in target sequence `Y`. Needed to\n",
    "        # compute the mask.\n",
    "        m = Y.size(0)\n",
    "\n",
    "        # Forward propagation of decoder. Use `dec_Q1`, `dec_K1` and\n",
    "        # `dec_V1`.\n",
    "        Q = self.dec_Q1(Y)\n",
    "        K = self.dec_K1(Y)\n",
    "        V = self.dec_V1(Y)\n",
    "\n",
    "        # Compute square upper triangular mask matrix of size `m`. You\n",
    "        # can use `torch.triu` and `torch.full` with `float(\"-inf\")`.\n",
    "        M = ...\n",
    "\n",
    "        # Score attention from `Q` and `K`. You can use `torch.bmm`\n",
    "        # and `transpose` but don't forget to add the mask `M`.\n",
    "        S = ...\n",
    "\n",
    "        # Attention\n",
    "        A = ...\n",
    "        T1 = ...\n",
    "\n",
    "        # First residual mapping and layer normalization\n",
    "        U1 = ...\n",
    "\n",
    "        # Key-value cross-attention using keys and values from the\n",
    "        # encoder.\n",
    "        Q = ...\n",
    "        K = ...\n",
    "        V = ...\n",
    "\n",
    "        # Score attention from `Q` and `K`. You can either use\n",
    "        # `torch.bmm` together with `torch.permute` or `torch.einsum`.\n",
    "        # S = torch.bmm(Q.permute([1, 0, 2]), K.permute([1, 2, 0])) / math.sqrt(self.p)\n",
    "        S = ...\n",
    "\n",
    "        # Attention\n",
    "        A = ...\n",
    "        T2 = ...\n",
    "\n",
    "        # Second residual mapping and layer normalization\n",
    "        U2 = ...\n",
    "\n",
    "        # FFN on each token\n",
    "        Z = ...\n",
    "\n",
    "        # Third residual mapping and layer normalization\n",
    "        U3 = ...\n",
    "\n",
    "        return U3"
   ],
   "id": "block-8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ],
   "id": "95b108e6-2d17-4ae4-89f5-7d507e588fa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, p=None, d_ff=None, vocab_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Declare an embedding, a positional encoder and a transformer\n",
    "        # encoder.\n",
    "        self.enc_embedding = nn.Embedding(vocab_size, p)\n",
    "        self.enc_positional_encoding = PositionalEncoding(p)\n",
    "        self.encoder = TransformerEncoder(p=p, d_ff=d_ff)\n",
    "\n",
    "        # Declare an embedding, a positional encoder and a transformer\n",
    "        # decoder.\n",
    "        self.dec_embedding = nn.Embedding(vocab_size, p)\n",
    "        self.dec_positional_encoding = PositionalEncoding(p)\n",
    "        self.decoder = TransformerDecoder(p=p, d_ff=d_ff)\n",
    "\n",
    "        self.generator = nn.Linear(p, vocab_size)\n",
    "\n",
    "    def encode(self, X):\n",
    "        # Use `self.enc_embedding`, `self.enc_positional_encoding` and\n",
    "        # `self.encoder` to compute `Xp`\n",
    "        X_emb = ...\n",
    "        X_emb_pos = ...\n",
    "        Xp = ...\n",
    "        return Xp\n",
    "\n",
    "    def decode(self, Xp, Y):\n",
    "        # Use `self.dec_embedding`, `self.dec_positional_encoding` and\n",
    "        # `self.decoder` to compute `outs`\n",
    "        Y_emb = ...\n",
    "        Y_emb_pos = ...\n",
    "        outs = ...\n",
    "        return outs\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        Xp = self.encode(X)\n",
    "        outs = self.decode(Xp, Y)\n",
    "        return self.generator(outs)\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, dataset: Dataset, optimizer: Optimizer):\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    # Set loss function to use. Don't forget to tell the loss function to\n",
    "    # ignore entries that are padded.\n",
    "    loss_fn = ...\n",
    "\n",
    "    # Turn `dataset` into an iterable on mini-batches using `DataLoader`.\n",
    "    train_dataloader = ...\n",
    "\n",
    "    losses = 0\n",
    "    for X, Y in train_dataloader:\n",
    "        # Select all but last element in sequences\n",
    "        Y_input = ...\n",
    "\n",
    "        # Resetting gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output of transformer from `X` and `Y_input`.\n",
    "        scores = ...\n",
    "\n",
    "        # Back-propagation through loss function\n",
    "        # Select all but first element in sequences\n",
    "        Y_output = ...\n",
    "\n",
    "        # Compute the cross-entropy loss between `scores` and\n",
    "        # `Y_output`. `scores` is `seq_length` * `batch_size` *\n",
    "        # `vocab_size` and contains scores and `Y_output` is\n",
    "        # `seq_length` * `batch_size` and contains integers. Two ways\n",
    "        # to compute the loss:\n",
    "        #\n",
    "        # - reshape both tensors to have `batch_size` * `probs` for `scores` and\n",
    "        #   `batch_size` for `Y_output`\n",
    "        # - permute dimensions to have `batch_size` * `vocab_size` *\n",
    "        #   `seq_length` for `scores` and `batch_size` * `seq_length` for\n",
    "        #   `Y_output`\n",
    "        loss = ...\n",
    "\n",
    "        # Gradient descent update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(dataset)"
   ],
   "id": "block-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval function"
   ],
   "id": "fb0de1af-f77b-4cd4-a6f5-25b7d77e7e2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, val_dataset: Dataset):\n",
    "    model.eval()\n",
    "\n",
    "    # Set loss function to use. Don't forget to tell the loss function to\n",
    "    # ignore entries that are padded.\n",
    "    loss_fn = ...\n",
    "\n",
    "    # Turn `val_dataset` into an iterable on mini-batches using `DataLoader`.\n",
    "    val_dataloader = ...\n",
    "\n",
    "    losses = 0\n",
    "    for X, Y in val_dataloader:\n",
    "        # Select all but last element in sequences\n",
    "        Y_input = ...\n",
    "\n",
    "        # Compute output of transformer from `X` and `Y_input`.\n",
    "        scores = ...\n",
    "\n",
    "        # Select all but first element in sequences\n",
    "        Y_output = ...\n",
    "\n",
    "        # Compute loss\n",
    "        loss = ...\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataset)"
   ],
   "id": "block-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning loop"
   ],
   "id": "766839df-8b5c-4ca1-92ff-6bec10139a77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    p=EMB_SIZE,\n",
    "    d_ff=FFN_HID_DIM,\n",
    "    vocab_size=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "optimizer = Adam(transformer.parameters())\n",
    "\n",
    "train_set = GotoDataset(n_sequences=N_SEQUENCES)\n",
    "test_set = GotoDataset(n_sequences=N_SEQUENCES)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, train_set, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer, test_set)\n",
    "    print(\n",
    "        (\n",
    "            f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )\n",
    "    )"
   ],
   "id": "block-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers functions"
   ],
   "id": "78cb6ad0-82a3-49e0-925c-ae73a41c6e42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, start_symbol_idx):\n",
    "    \"\"\"Autoregressive decoding of `src` starting with `start_symbol_idx`.\"\"\"\n",
    "\n",
    "    memory = model.encode(src)\n",
    "    ys = torch.LongTensor([[start_symbol_idx]])\n",
    "    maxlen = 100\n",
    "\n",
    "    for i in range(maxlen):\n",
    "        m = ys.size(0)\n",
    "        tgt_mask = torch.triu(torch.full((m, m), float(\"-inf\")), diagonal=1)\n",
    "\n",
    "        # Decode `ys`. `out` is of size `curr_len` * 1 * `vocab_size`\n",
    "        out = model.decode(memory, ys)\n",
    "\n",
    "        # Select encoding of last token\n",
    "        enc = out[-1, 0, :]\n",
    "\n",
    "        # Get a set of scores on vocabulary\n",
    "        dist = model.generator(enc)\n",
    "\n",
    "        # Get index of maximum\n",
    "        idx = torch.argmax(dist).item()\n",
    "\n",
    "        # Add predicted index to `ys`\n",
    "        ys = torch.cat((ys, torch.LongTensor([[idx]])))\n",
    "\n",
    "        if idx == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "def translate(model: torch.nn.Module, src_sentence: Iterable):\n",
    "    \"\"\"Translate sequence `src_sentence` with `model`.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Numericalize source\n",
    "    src_tensor = torch.LongTensor([tok2idx[tok] for tok in [\"<bos>\"] + list(src_sentence) + [\"<eos>\"]])\n",
    "\n",
    "    # Fake a minibatch of size one\n",
    "    src = src_tensor.unsqueeze(1)\n",
    "\n",
    "    # Translate `src`\n",
    "    tgt_tokens = greedy_decode(model, src, BOS_IDX)\n",
    "\n",
    "    tgt_tokens = tgt_tokens.flatten().numpy()\n",
    "    return \" \".join(idx2tok[idx] for idx in tgt_tokens[1:-1])\n",
    "\n",
    "\n",
    "input, output = dataset[2]\n",
    "\n",
    "print(\"Input:\", \" \".join(input))\n",
    "print(\"Output:\", \" \".join(output))\n",
    "print(\"Pred:\", translate(transformer, input))"
   ],
   "id": "block-12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements\n",
    "\n",
    "1.  Re-design transformer blocks into a more modern architecture by\n",
    "    inverting the order of the FFN and LayerNorm operations.\n",
    "\n",
    "2.  Extend the model by implementing Rotary Positional Embeddings (RoPE)\n",
    "    in the attention mechanism."
   ],
   "id": "ababa052-745b-40af-bc4d-4fbc4ef19e85"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "path": "/home/sylvain/.local/share/jupyter/kernels/python3"
  }
 }
}
